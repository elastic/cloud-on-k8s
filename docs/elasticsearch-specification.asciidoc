ifdef::env-github[]
****
link:https://www.elastic.co/guide/en/cloud-on-k8s/master/k8s-elasticsearch-specification.html[View this document on the Elastic website]
****
endif::[]
[id="{p}-elasticsearch-specification"]
== Running Elasticsearch on ECK

Before you deploy and run ECK, take some time to look at the basic and advanced settings available on this page. These settings are related both to Elasticsearch and Kubernetes.

**Basic settings**

- <<{p}-pod-template>>
- <<{p}-jvm-heap-size>>
- <<{p}-node-configuration>>
- <<{p}-volume-claim-templates>>
- <<{p}-http-settings-tls-sans>>

**Advanced settings**

- <<{p}-virtual-memory>>
- <<{p}-custom-http-certificate>>
- <<{p}-reserved-settings>>
- <<{p}-es-secure-settings>>
- <<{p}-bundles-plugins>>
- <<{p}-init-containers-plugin-downloads>>
- <<{p}-update-strategy>>
- <<{p}-pod-disruption-budget>>
- <<{p}-advanced-node-scheduling,Advanced Elasticsearch node scheduling>>
- <<{p}-orchestration>>
- <<{p}-snapshots,Create automated snapshots>>
- <<{p}-readiness>>
- <<{p}-prestop>>

[id="{p}-pod-template"]
=== Pod Template

Pod templates are the same you know and love from stateful sets and deployments. You can provide your own to add new settings, or merge settings with our defaults. For example, if you want to add new labels to your Pods, you can apply a Pod template as follows:

[source,yaml]
----
spec:
  nodeSets:
  - name: default
    count: 1
    podTemplate:
      metadata:
        labels:
          # additional labels for pods
          foo: bar
----

If you need to set resource requests and limits, see link:k8s-managing-compute-resources.html[Managing compute resources].

Or if you want to install additional plugins, see <<{p}-init-containers-plugin-downloads>>.

You might want to set environment variables to configure Elasticsearch. For example, to set the JVM heap size to `4g`, you can modify the environment variables of the `elasticsearch` container as follows:

[source,yaml]
----
spec:
  nodeSets:
  - name: default
    count: 1
    podTemplate:
      spec:
        containers:
        - name: elasticsearch
          env:
          - name: ES_JAVA_OPTS
            value: "-Xms4g -Xmx4g"
----

For more information on Pod templates, see the Kubernetes documentation:

- https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/#pod-templates[Pod Templates Overview]
- https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.15/#podtemplatespec-v1-core[Pod Template Spec API Reference]


[id="{p}-jvm-heap-size"]
=== JVM heap size

To change the heap size of Elasticsearch, set the `ES_JAVA_OPTS` environment variable in the `podTemplate`. It is also highly recommended to set the resource `requests` and `limits` at the same time to ensure that the pod gets enough resources allocated within the Kubernetes cluster. See <<{p}-compute-resources-elasticsearch>> for an example and more information.

If `ES_JAVA_OPTS` is not defined, the Elasticsearch default heap size of 1Gi will be in effect.

For more information, see the entry for `heap size` in the link:{ref}/important-settings.html[Important Elasticsearch configuration] documentation.


[id="{p}-node-configuration"]
=== Node configuration

Any setting defined in the `elasticsearch.yml` configuration file can also be defined for set of Elasticsearch nodes in the `spec.nodeSets[?].config` section.

[source,yaml]
----
spec:
  nodeSets:
  - name: masters
    count: 3
    config:
      node.master: true
      node.data: false
      node.ingest: false
      node.ml: false
      xpack.ml.enabled: true
      cluster.remote.connect: false
  - name: data
    count: 10
    config:
      node.master: false
      node.data: true
      node.ingest: true
      node.ml: true
      cluster.remote.connect: false
----

For more information on Elasticsearch settings, see https://www.elastic.co/guide/en/elasticsearch/reference/current/settings.html[Configuring Elasticsearch].

[id="{p}-volume-claim-templates"]
=== Volume claim templates

By default, the operator creates a https://kubernetes.io/docs/concepts/storage/persistent-volumes/[`PersistentVolumeClaim`] with a capacity of 1Gi for each pod in an Elasticsearch cluster to prevent data loss in case of accidental pod deletion. For production workloads, you should define your own volume claim template with the desired storage capacity and (optionally) the Kubernetes link:https://kubernetes.io/docs/concepts/storage/storage-classes/[storage class] to associate with the persistent volume. The name of the volume claim must always be `elasticsearch-data`.

[source,yaml]
----
spec:
  nodeSets:
  - name: default
    count: 3
    volumeClaimTemplates:
    - metadata:
        name: elasticsearch-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 5Gi
        storageClassName: standard
----

ECK automatically deletes PersistentVolumeClaim resources if they are not required for any Elasticsearch node. The corresponding PersistentVolume may be preserved, depending on the configured link:https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy[storage class reclaim policy].

IMPORTANT: Depending on the Kubernetes configuration and the underlying file system, some persistent volumes <<{p}-orchestration-limitations,cannot be resized after they are created>>. When you define volume claims, consider future storage requirements and make sure you have enough space to support the expected growth.

If you are not concerned about data loss, you can use an `emptyDir` volume for Elasticsearch data as well:

[source,yaml]
----
spec:
  nodeSets:
  - name: data
    count: 10
    podTemplate:
      spec:
        volumes:
        - name: elasticsearch-data
          emptyDir: {}
----

CAUTION: Using `emptyDir` is not recommended due to the high likelihood of permanent data loss.


[id="{p}-http-settings-tls-sans"]
=== HTTP settings & TLS SANs

In the `spec.http.service.spec` section, you can change the Kubernetes service used to expose Elasticsearch:

[source,yaml]
----
spec:
  http:
    service:
      spec:
        type: LoadBalancer
----

Check the https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types[Kubernetes Publishing Services (ServiceTypes)] that are currently available.

You can add an IP or a DNS name in the SAN of the self-signed certificate configured by default to secure the HTTP layer with TLS in the `spec.http.tls.selfSignedCertificate` section.

[source,yaml]
----
spec:
  http:
    tls:
      selfSignedCertificate:
        subjectAltNames:
        - ip: 1.2.3.4
        - dns: hulk.example.com
----

[id="{p}-virtual-memory"]
=== Virtual memory

By default, Elasticsearch uses memory mapping (`mmap`) to efficiently access indices.
Usually, default values for virtual address space on Linux distributions are too low for Elasticsearch to work properly, which may result in out-of-memory exceptions. This is why link:k8s-quickstart.html[the quickstart example] disables `mmap` via the `node.store.allow_mmap: false` setting. For production workloads, it is strongly recommended to increase the kernel setting `vm.max_map_count` to `262144` and leave `node.store.allow_mmap` unset.

The kernel setting `vm.max_map_count=262144` can be set on the host either directly or by a dedicated init container, which must be privileged. To add an init container that changes the host kernel setting before your Elasticsearch pod starts, you can use the following example Elasticsearch spec:
[source,yaml,subs="attributes,+macros"]
----
cat $$<<$$EOF | kubectl apply -f -
apiVersion: elasticsearch.k8s.elastic.co/{eck_crd_version}
kind: Elasticsearch
metadata:
  name: quickstart
spec:
  version: {version}
  nodeSets:
  - name: default
    count: 3
    config:
      node.master: true
      node.data: true
      node.ingest: true
    podTemplate:
      spec:
        initContainers:
        - name: sysctl
          securityContext:
            privileged: true
          command: ['sh', '-c', 'sysctl -w vm.max_map_count=262144']
EOF
----

Note that this requires the ability to run privileged containers, which is likely not the case on many secure clusters.

For more information, see the Elasticsearch documentation on
link:https://www.elastic.co/guide/en/elasticsearch/reference/current/vm-max-map-count.html[Virtual memory].

Optionally, you can select a different type of file system implementation for the storage. For possible options, see the
link:https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-store.html[store module documentation].

[source,yaml]
----
spec:
  nodeSets:
  - name: default
    count: 3
    config:
      index.store.type: niofs
----

[id="{p}-custom-http-certificate"]
=== Custom HTTP certificate

You can provide your own CA and certificates instead of the self-signed certificate to connect to Elasticsearch via HTTPS using a Kubernetes secret.
The certificate must be stored under `tls.crt` and the private key must be stored under `tls.key`. If your certificate was not issued by a well-known CA, you must include the trust chain under `ca.crt` as well.

You need to reference the name of a secret that contains a TLS private key and a certificate (and optionally, a trust chain), in the `spec.http.tls.certificate` section.

[source,yaml]
----
spec:
  http:
    tls:
      certificate:
        secretName: quickstart-es-cert
----

[float]
==== Custom self-signed certificate using OpenSSL

This example illustrates how to create your own self-signed certificate for the <<{p}-deploy-elasticsearch,quickstart Elasticsearch cluster>> using the OpenSSL command line utility. Note the subject alternative name (SAN) entry for `quickstart-es-http.default.svc`.

[source,sh]
----
$ openssl req -x509 -sha256 -nodes -newkey rsa:4096 -days 365 -subj "/CN=quickstart-es-http" -addext "subjectAltName=DNS:quickstart-es-http.default.svc" -keyout tls.key -out tls.crt
$ kubectl create secret generic quickstart-es-cert --from-file=ca.crt=tls.crt --from-file=tls.crt=tls.crt --from-file=tls.key=tls.key
----

[float]
==== Custom self-signed certificate using cert-manager

This example illustrates how to issue a self-signed certificate for the <<{p}-deploy-elasticsearch,quickstart Elasticsearch cluster>> using a link:https://cert-manager.io[cert-manager] self-signed issuer.

[source,yaml]
----
---
apiVersion: cert-manager.io/v1alpha2
kind: Issuer
metadata:
  name: selfsigned-issuer
spec:
  selfSigned: {}
---
apiVersion: cert-manager.io/v1alpha2
kind: Certificate
metadata:
  name: quickstart-es-cert
spec:
  isCA: true
  dnsNames:
    - quickstart-es-http
    - quickstart-es-http.default.svc
    - quickstart-es-http.default.svc.cluster.local
  issuerRef:
    kind: Issuer
    name: selfsigned-issuer
  secretName: quickstart-es-cert
----


[id="{p}-reserved-settings"]
=== Settings managed by ECK

The following Elasticsearch settings are managed by ECK:

* `cluster.name`
* `discovery.zen.minimum_master_nodes` deprecated[7.0]
* `cluster.initial_master_nodes` added[7.0]
* `network.host`
* `network.publish_host`
* `path.data`
* `path.logs`
* `xpack.security.authc.reserved_realm.enabled`
* `xpack.security.enabled`
* `xpack.security.http.ssl.certificate`
* `xpack.security.http.ssl.enabled`
* `xpack.security.http.ssl.key`
* `xpack.security.transport.ssl.certificate`
* `xpack.security.transport.ssl.enabled`
* `xpack.security.transport.ssl.key`
* `xpack.security.transport.ssl.verification_mode`

CAUTION: While ECK does not prevent you from setting any of these settings yourself, you are strongly discouraged from doing so and we cannot offer support for any user provided Elasticsearch configuration that does use any of these settings.


[id="{p}-es-secure-settings"]
=== Secure settings

You can specify link:https://www.elastic.co/guide/en/elasticsearch/reference/current/secure-settings.html[secure settings] with Kubernetes secrets.
The secrets should contain a key-value pair for each secure setting you want to add. ECK automatically injects these settings into the keystore on each Elasticsearch node before it starts Elasticsearch.

[source,yaml]
----
spec:
  secureSettings:
  - secretName: one-secure-settings-secret
  - secretName: two-secure-settings-secret
----

You can export a subset of secret keys and also project keys to specific paths using the `entries`, `key` and `path` fields:

[source,yaml]
----
spec:
  secureSettings:
  - secretName: your-secure-settings-secret
    entries:
    - key: key1
    - key: key2
      path: newkey2
----

See <<{p}-snapshots,How to create automated snapshots>> for an example use case.

[id="{p}-bundles-plugins"]
=== Custom configuration files and plugins

To run Elasticsearch with specific plugins or configuration files installed on ECK you have two options:

1. Create a custom Docker image with the plugins or files pre-installed.
2. Install the plugins or configuration files at Pod startup time.

NOTE: The first option has the advantage that you can verify the correctness of the image before rolling it out to your ECK installation, while the second option gives you
maximum flexibility. But the second option also means you might catch any errors only at runtime. Plugin installation at runtime has another drawback in that it needs access to the Internet from your cluster
and downloads each plugin multiple times, once for each Elasticsearch node.

See <<{p}-custom-images,Creating custom images>> for instructions on how to build custom Docker images based on the official Elastic images.

The following example describes option 2, using a repository plugin. To install the plugin before the Elasticsearch
nodes start, use an init container to run the link:https://www.elastic.co/guide/en/elasticsearch/plugins/current/installation.html[plugin installation tool].

[source,yaml]
----
spec:
  nodeSets:
  - name: default
    count: 3
    podTemplate:
      spec:
        initContainers:
        - name: install-plugins
          command:
          - sh
          - -c
          - |
            bin/elasticsearch-plugin install --batch repository-azure
----

To install custom configuration files you can use volumes and volume mounts.

The next example shows how to add a synonyms file for the
link:https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-synonym-tokenfilter.html[synonym token filter] in Elasticsearch.
But you can use the same approach for any kind of file you want to mount into the configuration directory of Elasticsearch.

[source,yaml]
----
spec:
  nodeSets:
  - name: default
    count: 3
    podTemplate:
      spec:
        containers:
        - name: elasticsearch <1>
          volumeMounts:
          - name: synonyms
            mountPath: /usr/share/elasticsearch/config/dictionaries
        volumes:
        - name: synonyms
          configMap:
            name: synonyms <2>
----

<1> Elasticsearch runs by convention in a container called 'elasticsearch'
<2> Assuming you have created a config map in the same namespace as Elasticsearch with the name 'synonyms' containing the synonyms file(s)

[id="{p}-init-containers-plugin-downloads"]
=== Init containers for plugin downloads

You can install custom plugins before the Elasticsearch container starts with an `initContainer`. For example:

[source,yaml]
----
spec:
  nodeSets:
  - name: default
    count: 3
    podTemplate:
      spec:
        initContainers:
        - name: install-plugins
          command:
          - sh
          - -c
          - |
            bin/elasticsearch-plugin install --batch repository-gcs
----

You can also override the Elasticsearch container image to use your own image with the plugins already installed, as described in the <<{p}-custom-images,custom images doc>>. The <<{p}-snapshots,snapshots>> document has more information on both these options. The Kubernetes document on https://kubernetes.io/docs/concepts/workloads/pods/init-containers/[init containers] has more information on their usage as well.

The init container inherits the image of the main container image if one is not explicitly set. It also inherits the volume mounts as long as the name and mount path do not conflict. It also inherits the Pod name and IP address environment variables.

[id="{p}-update-strategy"]
=== Update strategy

You can use the `updateStrategy` specification to limit the number of simultaneous changes, like for example in the following cases:

* The operator takes a Pod down to restart a node and applies a new configuration value.
* The operator must spin up Pods above what is currently in the specification to migrate to a new node set.

[source,yaml]
----
spec:
  updateStrategy:
    changeBudget:
      maxSurge: 3
      maxUnavailable: 1
----
`maxSurge`: Refers to the number of extra Pods that can be temporarily scheduled exceeding the number of Pods  defined in the specification. This setting is useful for controlling the resource usage of the Kubernetes cluster when new Pods need to be spun up to replace existing Pods. For example, when applying a configuration change to a Node Set, ECK creates a clone of the Node Set with the desired configuration and shuts down the old Pods. Without the `MaxSurge` restriction, the cluster would temporarily contain twice as many Pods as usual while this operation is in progress.

`maxUnavailable`: Refers to the number of Pods that can be unavailable out of the total number of Pods in the currently applied specification. A Pod is defined unavailable when it is not ready from a Kubernetes perspective.

The operator only tries to apply these constraints when a new specification is being applied. It is possible that the cluster state does not conform to the constraints at the beginning of the operation due to external factors. The operator will attempt to get to the desired state by adding or removing Pods as necessary while ensuring that the constraints are still satisfied.

For example, if a new specification defines a larger cluster with `maxUnavailable: 0`, the operator creates the missing Pods according to the best practices. Similarly, if a new specification defines a smaller cluster with `maxSurge: 0`, the operator safely removes the unnecessary Pods.

==== Specifying changeBudget
For both `maxSurge` and `maxUnavailable` you can specify the following values:

* `null` - The default value is used.
* non-negative - The value is used as is.
* negative - The value is unbounded.

==== Default behavior
When `updateStrategy` is not present in the specification, it defaults to the following:

[source,yaml]
----
spec:
  updateStrategy:
    changeBudget:
      maxSurge: -1
      maxUnavailable: 1
----

`maxSurge` is unbounded: This means that all the required Pods are created immediately.
`maxUnavailable` defaults to `1`: This ensures that the cluster has no more than one unavailable Pod at any given point in time.

==== Caveats
* With both `maxSurge` and `maxUnavailable` set to `0`, the operator cannot bring down an existing Pod nor create a new Pod.
* Due to the safety measures employed by the operator, certain `changeBudget`s might prevent the operator from making any progress . For example, with `maxSurge` set to 0, you cannot remove the last data node from one `nodeSet` and add a data node to a different `nodeSet`. In this case, the operator cannot create the new node because `maxSurge` is 0, and it cannot remove the old node because there are no other data nodes to migrate the data to.
* For certain complex configurations, the operator might not be able to deduce the optimal order of operations necessary to achieve the desired outcome. If progress is blocked,  you may need to update the `maxSurge` setting to a higher value than the theoretical best to help the operator make progress in that case.

If any of the above occurs, the operator generates logs to indicate that upscaling or downscaling are limited by `maxSurge` or `maxUnavailable` settings.

[id="{p}-pod-disruption-budget"]
=== Pod disruption budget

A link:https://kubernetes.io/docs/tasks/run-application/configure-pdb/[Pod Disruption Budget] allows limiting disruptions on an existing set of Pods while the Kubernetes cluster administrator manages Kubernetes nodes.
Elasticsearch makes sure some indices don't become unavailable.

A default PDB is enforced by default: it allows one Elasticsearch Pod to be taken down as long as the cluster has a `green` health.

This default can be tweaked in the Elasticsearch specification:

[source,yaml,subs="attributes"]
----
apiVersion: elasticsearch.k8s.elastic.co/{eck_crd_version}
kind: Elasticsearch
metadata:
  name: quickstart
spec:
  version: {version}
  nodeSets:
  - name: default
    count: 3
  podDisruptionBudget:
    spec:
      minAvailable: 2
      selector:
        matchLabels:
          elasticsearch.k8s.elastic.co/cluster-name: quickstart
----

Note that link:https://kubernetes.io/docs/tasks/run-application/configure-pdb/#arbitrary-controllers-and-selectors[`maxUnavailable` cannot be used with an arbitrary label selector], hence the usage of `minAvailable` in this example.

The default PDB can also be explicitly disabled:

[source,yaml,subs="attributes"]
----
apiVersion: elasticsearch.k8s.elastic.co/{eck_crd_version}
kind: Elasticsearch
metadata:
  name: quickstart
spec:
  version: {version}
  nodeSets:
  - name: default
    count: 3
  podDisruptionBudget: {}
----

include::orchestration.asciidoc[]
include::advanced-node-scheduling.asciidoc[]
include::snapshots.asciidoc[]


[id="{p}-readiness"]
=== Readiness probe

By default, the readiness probe checks that the pod can successfully respond to HTTP requests within a three second timeout. This is acceptable in most cases. In some cases (such as when the cluster is under heavy load), it may be helpful to increase the timeout. This allows the pod to stay in a `Ready` state and thus be part of the Elasticsearch service even if it is responding slowly. To adjust the timeout, set the `READINESS_PROBE_TIMEOUT` environment variable in the pod template. The readiness probe configuration also must be updated with the new timeout. For example, to increase the API call timeout to ten seconds and the overall check time to twelve seconds:

[source,yaml,subs="attributes"]
----
spec:
  version: {version}
  nodeSets:
    - name: default
      count: 1
      podTemplate:
        spec:
          containers:
          - name: elasticsearch
            readinessProbe:
              exec:
                command:
                - bash
                - -c
                - /mnt/elastic-internal/scripts/readiness-probe-script.sh
              failureThreshold: 3
              initialDelaySeconds: 10
              periodSeconds: 12
              successThreshold: 1
              timeoutSeconds: 12
            env:
            - name: READINESS_PROBE_TIMEOUT
              value: "10"


----

Note that this will require restarting the pods.

[id="{p}-prestop"]
=== Pod PreStop hook

When an Elasticsearch `Pod` is terminated, its `Endpoint` is removed from the `Service` and the Elasticsearch process is terminated. As these two operations happen in parallel, a race condition exists. If the Elasticsearch process is already shut down, but the `Endpoint` is still a part of the `Service`, any new connection might fail. For more information, see link:https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods[Termination of pods].

Moreover, kube-proxy resynchronizes its rules link:https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/#options[every 30 seconds by default]. During that 30 second time window, the terminating Pod IP may still be used when targeting the service. Please note the resync operation itself may take some time, especially if kube-proxy is configured to use iptables with a lot of services and rules to apply.

To address this issue and minimise unavailability, ECK relies on a link:https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/[PreStop lifecycle hook]. This waits to terminate the Elasticsearch process until the `Service` DNS record does not contain the IP of the `Pod`.
First, the PreStop lifecycle hook will keep querying DNS for `PRE_STOP_MAX_WAIT_SECONDS` (defaulting to 20) until the Pod IP is not referenced anymore.
Then, it waits for an additional `PRE_STOP_ADDITIONAL_WAIT_SECONDS` (defaulting to 30). Additional wait is used to:

1. give time to in-flight requests to be completed
2. give clients time to use the terminating Pod IP resolved just before DNS record was updated
3. give kube-proxy time to refresh ipvs or iptables rules on all nodes, depending on its sync period setting

The exact behavior is configurable using environment variables, for example:

[source,yaml,subs="attributes"]
----
spec:
  version: {version}
  nodeSets:
    - name: default
      count: 1
      podTemplate:
        spec:
          containers:
          - name: elasticsearch
            env:
            - name: PRE_STOP_MAX_WAIT_SECONDS
              value: "10"
            - name: PRE_STOP_ADDITIONAL_WAIT_SECONDS
              value: "5"
----
