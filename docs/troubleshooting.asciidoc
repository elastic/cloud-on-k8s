[id="{p}-troubleshooting"]
== Troubleshoot a failing cluster

When things don't seem to work as expected, there are several places you can look at to investigate.

[float]
[id="{p}-get-resources"]
=== Get the list of resources

To deploy and manage the Elastic stack, ECK creates several resources in the namespace the main resource is deployed in.

For example, each Elasticsearch node and Kibana instance has a dedicated Pod.
Check the Pods running and their status, to compare with the expected instances:

[source,sh]
----
kubectl get pods

NAME                                 READY     STATUS    RESTARTS   AGE
elasticsearch-sample-es-66sv6dvt7g   0/1       Pending   0          3s
elasticsearch-sample-es-9xzzhmgd4h   1/1       Running   0          27m
elasticsearch-sample-es-lgphkv9p67   0/1       Pending   0          3s
kibana-sample-kb-5468b8685d-c7mdp    0/1       Running   0          4s
----

Check the services:

[source,sh]
----
kubectl get services

NAME                           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
elasticsearch-sample-es-http   ClusterIP   10.19.248.93    <none>        9200/TCP   2d
kibana-sample-kb-http          ClusterIP   10.19.246.116   <none>        5601/TCP   3d
----

[float]
[id="{p}-describe-failing-resources"]
=== Describe failing resources

If an Elasticsearch node does not come up, it is possible the associated Pod cannot be scheduled by Kubernetes.
You may see this by looking at the Status of the Pods. If it does not become `Running` after a few seconds, chances are something's wrong:

[source,sh]
----
kubectl get pods

NAME                                 READY     STATUS    RESTARTS   AGE
elasticsearch-sample-es-66sv6dvt7g   0/1       Pending   0          3s
elasticsearch-sample-es-9xzzhmgd4h   1/1       Running   0          27m
elasticsearch-sample-es-lgphkv9p67   0/1       Pending   0          3s
kibana-sample-kb-5468b8685d-c7mdp    0/1       Running   0          4s
----

Get more details about the reason `elasticsearch-sample-es-lgphkv9p67` isn't scheduled:

[source,sh]
----
kubectl describe pod elasticsearch-sample-es-lgphkv9p67

(...)
Events:
  Type     Reason             Age               From                Message
  ----     ------             ----              ----                -------
  Warning  FailedScheduling   1m (x6 over 1m)   default-scheduler   pod has unbound immediate PersistentVolumeClaims (repeated 2 times)
  Warning  FailedScheduling   1m (x6 over 1m)   default-scheduler   pod has unbound immediate PersistentVolumeClaims
  Warning  FailedScheduling   1m (x11 over 1m)  default-scheduler   0/3 nodes are available: 1 node(s) had no available volume zone, 2 Insufficient memory.
  Normal   NotTriggerScaleUp  4s (x11 over 1m)  cluster-autoscaler  pod didn't trigger scale-up (it wouldn't fit if a new node is added)
----


[float]
[id="{p}-get-elasticsearch-logs"]
=== Get Elasticsearch logs

Each Elasticsearch node name is mapped to the corresponding Pod name.
To get the logs of a particular node, just fetch the Pod logs:

[source,sh]
----
kubectl logs -f elasticsearch-sample-es-lgphkv9p67

(...)
{"type": "server", "timestamp": "2019-07-22T08:48:10,859+0000", "level": "INFO", "component": "o.e.c.s.ClusterApplierService", "cluster.name": "elasticsearch-sample", "node.name": "elasticsearch-sample-es-lgphkv9p67", "cluster.uuid": "cX9uCx3uQrej9hMLGPhV0g", "node.id": "R_OcheBlRGeqme1IZzE4_Q",  "message": "added {{elasticsearch-sample-es-kqz4jmvj9p}{UGy5IX0UQcaKlztAoh4sLA}{3o_EUuZvRKW7R1C8b1zzzg}{10.16.2.232}{10.16.2.232:9300}{ml.machine_memory=27395555328, ml.max_open_jobs=20, xpack.installed=true},{elasticsearch-sample-es-stzz78k64p}{Sh_AzQcxRzeuIoOQWgru1w}{cwPoTFNnRAWtqsXWQtWbGA}{10.16.2.233}{10.16.2.233:9300}{ml.machine_memory=27395555328, ml.max_open_jobs=20, xpack.installed=true},}, term: 1, version: 164, reason: ApplyCommitRequest{term=1, version=164, sourceNode={elasticsearch-sample-es-9xzzhmgd4h}{tAi_bCPcSaO1OkLap4wmhQ}{E6VcWWWtSB2oo-2zmj9DMQ}{10.16.1.150}{10.16.1.150:9300}{ml.machine_memory=27395555328, ml.max_open_jobs=20, xpack.installed=true}}"  }
{"type": "server", "timestamp": "2019-07-22T08:48:22,224+0000", "level": "INFO", "component": "o.e.c.s.ClusterApplierService", "cluster.name": "elasticsearch-sample", "node.name": "elasticsearch-sample-es-lgphkv9p67", "cluster.uuid": "cX9uCx3uQrej9hMLGPhV0g", "node.id": "R_OcheBlRGeqme1IZzE4_Q",  "message": "added {{elasticsearch-sample-es-fn9wvxw6sh}{_tbAciHTStaAlUO6GtD9LA}{1g7_qsXwR0qjjfom05VwMA}{10.16.1.154}{10.16.1.154:9300}{ml.machine_memory=27395555328, ml.max_open_jobs=20, xpack.installed=true},}, term: 1, version: 169, reason: ApplyCommitRequest{term=1, version=169, sourceNode={elasticsearch-sample-es-9xzzhmgd4h}{tAi_bCPcSaO1OkLap4wmhQ}{E6VcWWWtSB2oo-2zmj9DMQ}{10.16.1.150}{10.16.1.150:9300}{ml.machine_memory=27395555328, ml.max_open_jobs=20, xpack.installed=true}}"  }
----

This can also be done for Kibana and APM Server.

[float]
[id="{p}-get-init-container-logs"]
=== Get init container logs

An Elasticsearch Pod runs a few init containers to prepare the file system of the main Elasticsearch container.
In some scenarios, the Pod may fail to run (`Status: Error` or `Status: CrashloopBackOff`) because one of the init containers is failing to run.
Look at the link:https://kubernetes.io/docs/tasks/debug-application-cluster/debug-init-containers/[init container statuses and logs to get more details].


[float]
[id="{p}-get-eck-logs"]
=== Get ECK logs

Since the ECK operator is just a standard Pod running in the Kubernetes cluster, you can fetch its logs as any other Pod:

[source,sh]
----
kubectl logs -f elastic-operator-0 -n elastic-system
----

The operator is constantly attempting to reconcile Kubernetes resources to match the desired state.
Logs with `INFO` level provide some insights about what is going on.

There should be normally very few logs with `ERROR` level: they indicate something is not going as expected.

Due to link:https://github.com/eBay/Kubernetes/blob/master/docs/devel/api-conventions.md#concurrency-control-and-consistency[optimistic locking],
there might be some errors reporting a conflict while updating a resource. Those are safe to ignore, since the update should go through at the next reconciliation attempt, which should happen almost immediately.

[float]
[id="{p}-eck-debug-logs"]
=== Enable ECK debug logs

To enable `DEBUG` level logs on the operator, restart it with the flag `--enable-debug-logs=true`. For example:

[source,sh]
----
kubectl edit statefulset.apps -n elastic-system elastic-operator
----

and change the following lines from:

[source,yaml]
----
  spec:
    containers:
    - args:
      - manager
      - --operator-roles
      - all
      - --enable-debug-logs
      - false
----

to

[source,yaml]
----
  spec:
    containers:
    - args:
      - manager
      - --operator-roles
      - all
      - --enable-debug-logs
      - true
----

[float]
[id="{p}-pause-controllers"]
=== Pause ECK controllers

When debugging Elasticsearch, it may be necessary to "pause" the operator reconciliations so no resource gets modified or created in the meantime.

It can be achieved by setting the annotation `common.k8s.elastic.co/pause` to `true` to any resource controlled by the operator :

- Elasticsearch
- Kibana
- ApmServer

[source,yaml]
----
metadata:
  annotations:
    common.k8s.elastic.co/pause: "true"
----

Or in one line:

[source,sh]
----
kubectl annotate elasticsearch quickstart --overwrite common.k8s.elastic.co/pause=true
----

[float]
[id="{p}-get-k8s-events"]
=== Get Kubernetes events

ECK will emit some events when:

* important operations are performed (example: a new Elasticsearch Pod was created)
* something is wrong, and the user should know about it

Fetch Kubernetes events:

[source,sh]
----
kubectl get events

(...)
28s       25m       58        elasticsearch-sample-es-p45nrjch29.15b3ae4cc4f7c00d   Pod                             Warning   FailedScheduling    default-scheduler                                         0/3 nodes are available: 1 node(s) had no available volume zone, 2 Insufficient memory.
28s       25m       52        elasticsearch-sample-es-wxpnzfhqbt.15b3ae4d86bc269f   Pod                             Warning   FailedScheduling    default-scheduler                                         0/3 nodes are available: 1 node(s) had no available volume zone, 2 Insufficient memory.
----

The list of events can be filtered to a particular Elasticsearch cluster:

[source,sh]
----
kubectl get event --namespace default --field-selector involvedObject.name=elasticsearch-sample

LAST SEEN   FIRST SEEN   COUNT     NAME                                    KIND            SUBOBJECT   TYPE      REASON    SOURCE                     MESSAGE
30m         30m          1         elasticsearch-sample.15b3ae303baa93c0   Elasticsearch               Normal    Created   elasticsearch-controller   Created pod elasticsearch-sample-es-4q7q2k8cl7
30m         30m          1         elasticsearch-sample.15b3ae303bab4f40   Elasticsearch               Normal    Created   elasticsearch-controller   Created pod elasticsearch-sample-es-jg7dsfkcp8
30m         30m          1         elasticsearch-sample.15b3ae303babdfc8   Elasticsearch               Normal    Created   elasticsearch-controller   Created pod elasticsearch-sample-es-xrxsp54jd5
----

This can also be done for Kibana and APM Server.

[float]
[id="{p}-exec-into-containers"]
=== Exec into containers

To troubleshoot a filesystem, configuration or network issue, it is sometimes useful to execute shell commands in the Elasticsearch container directly.
This can be done with `kubectl`:

[source,sh]
----
kubectl exec -ti elasticsearch-sample-es-p45nrjch29 bash
----

This can also be done for Kibana and APM Server.

[float]
[id="{p}-ask-for-help"]
=== Ask for help

* link:https://discuss.elastic.co/c/eck[ECK Discuss forums] to ask any question
* link:https://github.com/elastic/cloud-on-k8s/issues[Github issues] for bugs and features
