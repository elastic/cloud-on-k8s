[id="{p}-elasticsearch-specification"]
== Running Elasticsearch on ECK

Before you deploy and run ECK, take some time to look at the basic and advanced settings available on this page. These settings are related both to Elasticsearch and Kubernetes.

**Basic settings**

- <<{p}-pod-template>>
- <<{p}-jvm-heap-size>>
- <<{p}-node-configuration>>
- <<{p}-volume-claim-templates>>
- <<{p}-http-settings-tls-sans>>

**Advanced settings**

- <<{p}-virtual-memory>>
- <<{p}-custom-http-certificate>>
- <<{p}-es-secure-settings>>
- <<{p}-bundles-plugins>>
- <<{p}-init-containers-plugin-downloads>>
- <<{p}-update-strategy>>
- <<{p}-pod-disruption-budget>>
- <<{p}-advanced-node-scheduling,Advanced Elasticsearch node scheduling>>
- <<{p}-orchestration>>
- <<{p}-snapshot,Create automated snapshots>>

[id="{p}-pod-template"]
=== Pod Template

Pod templates are the same you know and love from stateful sets and deployments. You can provide your own to add new settings, or merge settings with our defaults. For example, if you want to add new labels to your Pods, you can apply a Pod template as follows:

[source,yaml]
----
    podTemplate:
      metadata:
        labels:
          # additional labels for pods
          foo: bar
----

If you need to set resource requests and limits, see link:k8s-managing-compute-resources.html[Managing compute resources].

Or if you want to install additional plugins, see <<{p}-init-containers-plugin-downloads>>.

You might want to set environment variables to configure Elasticsearch. For example, to set the minimum and maximum JVM heap size to `2g` and `4g` respectively, you can modify the environment variables of the `elasticsearch` container as follows:

[source,yaml]
----
spec:
  podTemplate:
    spec:
      containers:
      - name: elasticsearch
        env:
        - name: ES_JAVA_OPTS
          value: "-Xms2g -Xmx4g"
----

For more information on Pod templates, see the Kubernetes documentation:

- https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/#pod-templates[Pod Templates Overview]
- https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.15/#podtemplatespec-v1-core[Pod Template Spec API Reference]


[id="{p}-jvm-heap-size"]
=== JVM heap size

To change the heap size of Elasticsearch, set the `ES_JAVA_OPTS` environment variable in the `podTemplate`. It is also highly recommended to set the resource `requests` and `limits` at the same time to ensure that the pod gets enough resources allocated within the Kubernetes cluster. See <<{p}-compute-resources-elasticsearch>> for an example and more information.

If `ES_JAVA_OPTS` is not defined, the Elasticsearch default heap size of 1Gi will be in effect.

See also: link:https://www.elastic.co/guide/en/elasticsearch/reference/current/heap-size.html[Elasticsearch documentation on setting the heap size]


[id="{p}-node-configuration"]
=== Node configuration

Any setting defined in the `elasticsearch.yml` configuration file can also be defined for each topology of nodes in the `spec.nodeSets[?].config` section.

[source,yaml]
----
spec:
  nodeSets:
  - name: masters
    count: 3
    config:
      node.master: true
      node.data: false
      node.ingest: false
      node.ml: false
      xpack.ml.enabled: true
      cluster.remote.connect: false
  - name: data
    count: 10
    config:
      node.master: false
      node.data: true
      node.ingest: true
      node.ml: true
      cluster.remote.connect: false
----

For more information on Elasticsearch settings, see https://www.elastic.co/guide/en/elasticsearch/reference/current/settings.html[Configuring Elasticsearch].

[id="{p}-volume-claim-templates"]
=== Volume claim templates

By default, the operator creates a https://kubernetes.io/docs/concepts/storage/persistent-volumes/[`PersistentVolumeClaim`] with a capacity of 1Gi for each pod in an Elasticsearch cluster to prevent data loss in case of accidental pod deletion. For production workloads, you should define your own volume claim template with the desired storage capacity and (optionally) the Kubernetes link:https://kubernetes.io/docs/concepts/storage/storage-classes/[storage class] to associate with the persistent volume. The name of the volume claim must always be `elasticsearch-data`.

[source,yaml]
----
spec:
  nodeSets:
  - name: default
    count: 3
    volumeClaimTemplates:
    - metadata:
        name: elasticsearch-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 5Gi
        storageClassName: standard
----

ECK automatically deletes VolumeClaimTemplates resources if they are not required for any Elasticsearch node. The corresponding PersistentVolume may be preserved, depending on the configured link:https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy[storage class reclaim policy].

IMPORTANT: Depending on the Kubernetes configuration and the underlying file system, some persistent volumes <<{p}-orchestration-limitations,cannot be resized after they are created>>. When you define volume claims, consider future storage requirements and make sure you have enough space to support the expected growth.

If you are not concerned about data loss, you can use an `emptyDir` volume for Elasticsearch data as well:

[source,yaml]
----
spec:
  nodeSets:
  - config:
    podTemplate:
      spec:
        volumes:
        - name: elasticsearch-data
          emptyDir: {}
----

CAUTION: Using `emptyDir` is not recommended due to the high likelihood of permanent data loss.


[id="{p}-http-settings-tls-sans"]
=== HTTP settings & TLS SANs

In the `spec.http.service.spec` section, you can change the Kubernetes service used to expose Elasticsearch:

[source,yaml]
----
spec:
  http:
    service:
      spec:
        type: LoadBalancer
----

Check the https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types[Kubernetes Publishing Services (ServiceTypes)] that are currently available.

You can add an IP or a DNS name in the SAN of the self-signed certificate configured by default to secure the HTTP layer with TLS in the `spec.http.tls.selfSignedCertificate` section.

[source,yaml]
----
spec:
  http:
    tls:
      selfSignedCertificate:
        subjectAltNames:
        - ip: 1.2.3.4
        - dns: hulk.example.com
----

[id="{p}-virtual-memory"]
=== Virtual memory

By default, Elasticsearch uses memory mapping (`mmap`) to efficiently access indices.
Usually, default values for virtual address space on Linux distributions are too low for Elasticsearch to work properly, which may result in out-of-memory exceptions. This is why link:k8s-quickstart.html[the quickstart example] disables `mmap` via the `node.store.allow_mmap: false` setting. For production workloads, it is strongly recommended to increase the kernel setting `vm.max_map_count` to `2621441` and leave `node.store.allow_mmap` unset.

The kernel setting `vm.max_map_count=2621441` can be set on the host either directly or by a dedicated init container, which must be privileged. To add an init container that changes the host kernel setting before your Elasticsearch pod starts, you can use the following example Elasticsearch spec:
[source,yaml]
----
cat <<EOF | kubectl apply -f -
apiVersion: elasticsearch.k8s.elastic.co/v1beta1
kind: Elasticsearch
metadata:
  name: quickstart
spec:
  version: 7.4.0
  nodes:
  - name: default
    nodeCount: 1
    config:
      node.master: true
      node.data: true
      node.ingest: true
    podTemplate:
      spec:
        initContainers:
        - name: sysctl
          securityContext:
            privileged: true
          command: ['sh', '-c', 'sysctl -w vm.max_map_count=262144']
EOF
----

Note that this requires the ability to run privileged containers, which is likely not the case on many secure clusters.

For more information, see the Elasticsearch documentation on
link:https://www.elastic.co/guide/en/elasticsearch/reference/current/vm-max-map-count.html[Virtual memory].

Optionally, you can select a different type of file system implementation for the storage. For possible options, see the
link:https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-store.html[store module documentation].

[source,yaml]
----
spec:
  nodeSets:
  - name: default
    count: 3
    config:
      index.store.type: niofs
----

[id="{p}-custom-http-certificate"]
=== Custom HTTP certificate

You can provide your own CA and certificates instead of the self-signed certificate to connect to Elasticsearch via HTTPS using a Kubernetes secret.
The certificate must be stored under `tls.crt` and the private key must be stored under `tls.key`. If your certificate was not issued by a well-known CA, you must include the trust chain under `ca.crt` as well.

You need to reference the name of a secret that contains a TLS private key and a certificate (and optionally, a trust chain), in the `spec.http.tls.certificate` section.

[source,yaml]
----
spec:
  http:
    tls:
      certificate:
        secretName: my-cert
----

This is an example on how create a Kubernetes TLS secret with a self-signed certificate:

[source,sh]
----
$ openssl req -x509 -sha256 -nodes -newkey rsa:4096 -days 365 -subj "/CN=quickstart-es-http" -addext "subjectAltName=DNS:quickstart-es-http.default.svc" -keyout tls.key -out tls.crt
$ kubectl create secret generic my-cert --from-file=ca.crt=tls.crt --from-file=tls.crt=tls.crt --from-file=tls.key=tls.key
----

[id="{p}-es-secure-settings"]
=== Secure settings

You can specify link:https://www.elastic.co/guide/en/elasticsearch/reference/current/secure-settings.html[secure settings] with Kubernetes secrets.
The secrets should contain a key-value pair for each secure setting you want to add. Reference that secrets in the Elasticsearch
resource specification for ECK to automatically inject those settings into the keystore on each node before it starts Elasticsearch.

[source,yaml]
----
spec:
  secureSettings:
  - secretName: one-secure-settings-secret
  - secretName: two-secure-settings-secret
----

You can export a subset of secret keys and also project keys to specific paths using the `entries`, `key` and `path` fields:

[source,yaml]
----
spec:
  secureSettings:
  - secretName: your-secure-settings-secret
    entries:
    - key: key1
    - key: key2
      path: newkey2
----

See link:k8s-snapshot.html[How to create automated snapshots] for an example use case.

[id="{p}-bundles-plugins"]
=== Custom configuration files and plugins

To run Elasticsearch with specific plugins or configuration files installed on ECK you have two options:

1. Create a custom Docker image with the plugins or files pre-installed.
2. Install the plugins or configuration files at Pod startup time.

NOTE: The first option has the advantage that you can verify the correctness of the image before rolling it out to your ECK installation, while the second option gives you
maximum flexibility. But the second option also means you might catch any errors only at runtime. Plugin installation at runtime has another drawback in that it needs access to the Internet from your cluster
and downloads each plugin multiple times, once for each Elasticsearch node.

Building your custom Docker images is outside the scope of this documentation despite being the better solution for most users.

The following example describes option 2, using a repository plugin. To install the plugin before the Elasticsearch
nodes start, use an init container to run the link:https://www.elastic.co/guide/en/elasticsearch/plugins/current/installation.html[plugin installation tool].

[source,yaml]
----
spec:
  nodeSets:
  - podTemplate:
      spec:
        initContainers:
        - name: install-plugins
          command:
          - sh
          - -c
          - |
            bin/elasticsearch-plugin install --batch repository-azure
----

To install custom configuration files you can use volumes and volume mounts.

The next example shows how to add a synonyms file for the
link:https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-synonym-tokenfilter.html[synonym token filter] in Elasticsearch.
But you can use the same approach for any kind of file you want to mount into the configuration directory of Elasticsearch.

[source,yaml]
----
spec:
  nodeSets:
  - podTemplate:
      spec:
        containers:
        - name: elasticsearch <1>
          volumeMounts:
          - name: synonyms
            mountPath: /usr/share/elasticsearch/config/dictionaries
        volumes:
        - name: synonyms
          configMap:
            name: synonyms <2>
----

<1> Elasticsearch runs by convention in a container called 'elasticsearch'
<2> Assuming you have created a config map in the same namespace as Elasticsearch with the name 'synonyms' containing the synonyms file(s)

[id="{p}-init-containers-plugin-downloads"]
=== Init containers for plugin downloads

You can install custom plugins before the Elasticsearch container starts with an `initContainer`. For example:

[source,yaml]
----
  - podTemplate:
      spec:
        initContainers:
        - name: install-plugins
          command:
          - sh
          - -c
          - |
            bin/elasticsearch-plugin install --batch repository-gcs
----

You can also override the Elasticsearch container image to use your own image with the plugins already installed, as described in the link:k8s-images.asciidoc[custom images doc]. The link:k8s-snapshot.html[snapshots] doc has more information on both these options. The Kubernetes doc on https://kubernetes.io/docs/concepts/workloads/pods/init-containers/[init containers] has more information on their usage as well.

The init container inherits the image of the main container image if one is not explicitly set. It also inherits the volume mounts as long as the name and mount path do not conflict. It also inherits the Pod name and IP address environment variables.

[id="{p}-update-strategy"]
=== Update strategy

The Elasticsearch cluster configuration can be updated at any time to:

* Add new nodes
* Remove some nodes
* Change Elasticsearch configuration
* Change Pod resources (memory limits, cpu limit, environment variables, etc.)

On any change, ECK reconciles Kubernetes resources towards the desired cluster definition. Changes occur in a rolling fashion: the state of the cluster is continuously monitored, to allow addition of new nodes and removal of deprecated nodes.

[id="{p}-change-budget"]
==== Change budget

No downtime should be expected when the cluster topology changes. Shards on deprecated nodes are migrated away so the node can be safely removed.

For example, to mutate a 3-nodes cluster with 16GB memory limit on each node to a 3-nodes cluster with 32GB memory limit on each node, ECK will:

1. Add a new 32GB node: the cluster temporarily has 4 nodes
2. Migrate data away from the first 16GB node
3. Once data is migrated, remove the first 16GB node
4. Follow the same steps for the 2 other 16GB nodes

The cluster health stays green during the entire process.
By default, only one extra node can be added on top of the expected ones. In the example above, a 3-nodes cluster may temporarily be composed of 4 nodes while data migration is in progress.

This behaviour can be controlled through the `changeBudget` section of the cluster specification `updateStrategy`. If not specified, it defaults to the following:

[source,yaml]
----
spec:
  updateStrategy:
    changeBudget:
      maxSurge: 1
      maxUnavailable: 0
----

* `maxSurge` specifies the number of Pods that can be added to the cluster, on top of the desired number of nodes in the specification during cluster updates
* `maxUnavailable` specifies the number of Pods that can be made unavailable during cluster updates

The default of `maxSurge: 1; maxUnavailable: 0` spins up an additional Elasticsearch node during cluster updates.
It is possible to speed up cluster topology changes by increasing `maxSurge`. For example, setting `maxSurge: 3` would allow 3 new nodes to be created while the original 3 migrate data in parallel.
The cluster would then temporarily have 6 nodes.

Setting `maxSurge` to 0 and `maxUnavailable` to a positive value only allows a maximum number of Pods to exist on the Kubernetes cluster.
For example, `maxSurge: 0; maxUnavailable: 1` would perform the 3 nodes upgrade this way:

1. Migrate data away from the first 16GB node
2. Once data is migrated, remove the 16GB node: the cluster temporarily has 2 nodes
3. Add a new 32GB node: the cluster grows to 3 nodes
4. Follow the same steps for the 2 other 16GB nodes

Even if a `changeBudget` is specified, ECK makes sure that some invariants are maintained while a mutation is in progress. In the cluster, there must be at least:

* One master node alive
* One data node alive

So under certain circumstances ECK ignores the change budget. For example, a safe migration from a 1-node cluster to another 1-node cluster can only be done by temporarily setting up a 2-nodes cluster.

It is possible to configure the `changeBudget` to optimize the reuse of persistent volumes, instead of migrating data across nodes. This feature is not supported yet, more details to come in the next release.

[id="{p}-pod-disruption-budget"]
=== Pod disruption budget

A link:https://kubernetes.io/docs/tasks/run-application/configure-pdb/[Pod Disruption Budget] allows limiting disruptions on an existing set of Pods while the Kubernetes cluster administrator manages cluster nodes.
Elasticsearch makes sure some indices don't become unavailable.

A default PDB of 1 `maxUnavailable` Pod on the entire cluster is enforced by default.

This default can be tweaked in the Elasticsearch specification:

[source,yaml,subs="attributes"]
----
apiVersion: elasticsearch.k8s.elastic.co/{eck_crd_version}
kind: Elasticsearch
metadata:
  name: quickstart
spec:
  version: {version}
  nodeSets:
  - name: default
    count: 3
  podDisruptionBudget:
    spec:
      maxUnavailable: 2
      selector:
        matchLabels:
          elasticsearch.k8s.elastic.co/cluster-name: quickstart
----

It can also be explicitly disabled:

[source,yaml,subs="attributes"]
----
apiVersion: elasticsearch.k8s.elastic.co/{eck_crd_version}
kind: Elasticsearch
metadata:
  name: quickstart
spec:
  version: {version}
  nodeSets:
  - name: default
    count: 3
  podDisruptionBudget: {}
----

include::orchestration.asciidoc[]
include::advanced-node-scheduling.asciidoc[]
include::snapshots.asciidoc[]
