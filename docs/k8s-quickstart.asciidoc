[id="{p}-quickstart"]

= Quickstart

[partintro]
--
With Elastic Cloud on Kubernetes you can extend the basic Kubernetes orchestration capabilities to easily deploy, secure, upgrade your Elasticsearch cluster, and much more.

Eager to get started? This fast guide shows you how to:

* <<{p}-deploy-operator,Deploy the operator in your Kubernetes cluster>>
* <<{p}-deploy-elasticsearch,Deploy the Elasticsearch cluster>>
* <<{p}-deploy-kibana,Deploy the Kibana instance>>
* <<{p}-upgrade-deployment,Upgrade your deployment>>
* <<{p}-deep-dive,Deep dive>>

**Requirements**

This quickstart assumes you already have Kubernetes 1.11+.
--

[id="{p}-deploy-operator"]
== Deploy the operator in your Kubernetes cluster

NOTE: If you are using GKE, make sure your user has `cluster-admin` permissions. For more information, see link:https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control[Prerequisites for using Kubernetes RBAC on GKE].

. Install link:https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/[custom resource definitions], to extend the API server with additional resources:

  kubectl apply -f https://raw.githubusercontent.com/elastic/cloud-on-k8s/master/operators/config/crds.yaml

. Install the operator with its RBAC rules:

  kubectl apply -f https://raw.githubusercontent.com/elastic/cloud-on-k8s/master/operators/config/all-in-one.yaml

. Monitor the operator logs:

  kubectl -n elastic-system logs -f statefulset.apps/elastic-operator

[float]
[id="{p}-deploy-elasticsearch"]
== Deploy the Elasticsearch cluster

Apply a simple link:{ref}getting-started.html[Elasticsearch] cluster specification, with one node:

----
cat <<EOF | kubectl apply -f -
apiVersion: elasticsearch.k8s.elastic.co/v1alpha1
kind: Elasticsearch
metadata:
  name: quickstart
spec:
  version: 7.0.0
  nodes:
  - nodeCount: 1
    config:
      node.master: true
      node.data: true
      node.ingest: true
      xpack.license.self_generated.type: trial
EOF
----

The operator automatically takes care of managing Pods and resources corresponding to the desired cluster. It may take up to a few minutes until the cluster is ready.

[float]
=== Monitor cluster health and creation progress

Get an overview of the current Elasticsearch clusters in the Kubernetes cluster, including health, version and number of nodes:

`kubectl get elasticsearch`

----
NAME          HEALTH    NODES     VERSION   PHASE         AGE
quickstart    green     1         7.0.0     Operational   1m
----

When you create the cluster, there is no `HEALTH` status and the `PHASE` is `Pending`. After a while, the `PHASE` turns into `Operational`, and `HEALTH` becomes `green`.

You can see that one Pod is in the process of being started:

`kubectl get pods --selector='elasticsearch.k8s.elastic.co/cluster-name=quickstart'`

----
NAME                       READY     STATUS    RESTARTS   AGE
quickstart-es-5zctxpn8nd   1/1       Running   0          1m
----

And access the logs for that Pod:

`kubectl logs -f quickstart-es-5zctxpn8nd`

[float]
=== Request Elasticsearch access

A ClusterIP Service is automatically created for your cluster:

`kubectl get service quickstart-es`

. Get access to elasticsearch
To access Elasticsearch from the Kubernetes cluster, use the URL `https://quickstart-es:9200`.

----
NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
quickstart-es   ClusterIP   10.15.251.145   <none>        9200/TCP   34m
----

To access Elasticsearch from your local workstation, use the following command:

   kubectl port-forward service/quickstart-es 9200

In another shell, get the credentials and request the Elasticsearch endpoint.

A default user named `elastic` is automatically created. Its password is stored as a Kubernetes secret:

  PASSWORD=$(kubectl get secret quickstart-elastic-user -o=jsonpath='{.data.elastic}' | base64 --decode)
+
Request the Elasticsearch endpoint:

    curl -u "elastic:$PASSWORD" -k "https://localhost:9200"
+
NOTE: For testing purposes only, you can specify the `-k` option to turn off certificate verification.
+
----
    {
      "name" : "quickstart-es-5zctxpn8nd",
      "cluster_name" : "quickstart",
      "cluster_uuid" : "2sUV1IUEQ5SA5ZSkhznCHA",
      "version" : {
        "number" : "7.0.0",
        "build_flavor" : "default",
        "build_type" : "docker",
        "build_hash" : "b7e28a7",
        "build_date" : "2019-04-05T22:55:32.697037Z",
        "build_snapshot" : false,
        "lucene_version" : "8.0.0",
        "minimum_wire_compatibility_version" : "6.7.0",
        "minimum_index_compatibility_version" : "6.0.0-beta1"
      },
      "tagline" : "You Know, for Search"
    }
----

[float]
[id="{p}-deploy-kibana"]
== Deploy the Kibana instance

To deploy your link:{kibana-ref}introduction.html#introduction[Kibana] instance go through the following steps.

. Specify a Kibana instance and associate it with your Elasticsearch cluster:

  ----
  cat <<EOF | kubectl apply -f -
  apiVersion: kibana.k8s.elastic.co/v1alpha1
  kind: Kibana
  metadata:
    name: quickstart
    spec:
    version: 7.0.0
    nodeCount: 1
    ---
    apiVersion: associations.k8s.elastic.co/v1alpha1
    kind: KibanaElasticsearchAssociation
    metadata:
    name: kibana-es-quickstart
    spec:
    elasticsearch:
      name: quickstart
      namespace: default
      kibana:
      name: quickstart
      namespace: default
      EOF
      ----

. Monitor Kibana health and creation progress
+
Similar to Elasticsearch, you can retrieve some details about Kibana instances:

  kubectl get kibana
+
And the associated Pods:

  kubectl get pod --selector='kibana.k8s.elastic.co/name=quickstart'

. Access Kibana
+
A `ClusterIP` Service is automatically created for Kibana:

  kubectl get service quickstart-kibana
+
NOTE: You need the elastic password. Retrieve it again and copy it:

  PASSWORD=$(kubectl get secret quickstart-elastic-user -o=jsonpath='{.data.elastic}' | base64 --decode)

  echo $PASSWORD
+
Use `kubectl port-forward` to access Kibana from your local workstation:

  kubectl port-forward service/quickstart-kibana 5601
+
Open `http://localhost:5601` in your browser.

[float]
[id="{p}-upgrade-deployment"]
== Upgrade your deployment

You can apply any modification to the original cluster specification. The operator makes sure that your changes are applied to the existing cluster, by avoiding downtime.

For example, you can grow the cluster to three nodes:

[source,sh]
----
cat <<EOF | kubectl apply -f -
apiVersion: elasticsearch.k8s.elastic.co/v1alpha1
kind: Elasticsearch
metadata:
  name: quickstart
spec:
  version: 7.0.0
  nodes:
  - nodeCount: 3
    config:
      node.master: true
      node.data: true
      node.ingest: true
      xpack.license.self_generated.type: trial
EOF
----

[float]
[id="{p}-deep-dive"]
== Deep dive

Now that you have completed the quickstart, you can try out more features like securing your cluster or using persistent storage.

[float]
=== Secure your cluster

To secure your production-grade Elasticsearch deployment, you can:

*  Use XPack security for encryption and authentication

(TODO: link here to a tutorial on how to manipulate certs and auth. Note from nrichers: X-Pack [sic] is going away, so this should just talk about the security features of the Elastic Stack. See https://docs.google.com/document/d/1GX6IzKDf8IBTQexcSZZj_C-ryH4FzsSVf7s0SHKiLKA/edit# for more info.)

*  Set up an ingress proxy layer (link:https://github.com/elastic/cloud-on-k8s/blob/master/operators/config/samples/ingress/nginx-ingress.yaml[example using NGINX])

[float]
=== Use persistent storage

The cluster that you deployed in this quickstart uses an link:https://kubernetes.io/docs/concepts/storage/volumes/#emptydir[emptyDir volume], which might not qualify for production workloads.

You can request a `PersistentVolumeClaim` in the cluster specification, to target any `PersistentVolume` class available in your Kubernetes cluster:

----
yaml
apiVersion: elasticsearch.k8s.elastic.co/v1alpha1
kind: Elasticsearch
metadata:
  name: my-cluster
spec:
  version: 7.0.0
  nodes:
  - nodeCount: 3
    config:
      node.master: true
      node.data: true
      node.ingest: true
      xpack.license.self_generated.type: trial
    volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 100GB
        storageClassName: gcePersistentDisk # can be any available storage class
----

To aim for the best performance, the operator supports persistent volumes local to each node. For more details, see:

 * link:https://github.com/elastic/cloud-on-k8s/tree/master/local-volume[elastic local volume dynamic provisioner]to setup dynamic local volumes based on LVM.
 * link:https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner[kubernetes-sigs local volume static provisioner] to setup static local volumes.

[float]
=== Additional features

The operator supports the following features:

* Node-to-node TLS encryption
* User management
* Secure settings (for ex. automated snapshots)
* Nodes resources limitations (CPU, RAM, disk)
* Cluster update strategies
* Version upgrades
* Node attributes
* Cross-cluster search and replication
* Licensing
* Operator namespace management
* APM server deployments
* Pausing reconciliations
* Full cluster restart
