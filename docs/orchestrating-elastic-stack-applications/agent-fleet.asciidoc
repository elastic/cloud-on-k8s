:page_id: elastic-agent-fleet
:agent_recipes: https://raw.githubusercontent.com/elastic/cloud-on-k8s/{eck_release_branch}/config/recipes/elastic-agent
ifdef::env-github[]
****
link:https://www.elastic.co/guide/en/cloud-on-k8s/master/k8s-{page_id}.html[View this document on the Elastic website]
****
endif::[]
[id="{p}-{page_id}"]
= Run {fleet}-managed {agent} on ECK

This section describes how to configure and deploy {agent} in link:{fleet-guide}/elastic-agent-installation.html[{fleet}-managed] mode with ECK. Check the link:k8s-elastic-agent.html[Standalone section] if you want to run {agent} in the link:{fleet-guide}/install-standalone-elastic-agent.html[standalone mode].

* <<{p}-elastic-agent-fleet-quickstart,Quickstart>>
* <<{p}-elastic-agent-fleet-configuration,Configuration>>
* <<{p}-elastic-agent-fleet-configuration-examples,Configuration Examples>>
* <<{p}-elastic-agent-fleet-known-limitations,Known Limitations>>

[id="{p}-elastic-agent-fleet-quickstart"]
== Quickstart

. To deploy {fleet-server}, {agents}, {es}, and {kib}, apply the following specification:
+
[source,yaml,subs="attributes,callouts,+macros"]
----
cat $$<<$$EOF | kubectl apply -f -
apiVersion: agent.k8s.elastic.co/v1alpha1
kind: Agent
metadata:
  name: fleet-server-quickstart
  namespace: default
spec:
  version: {version}
  kibanaRef:
    name: kibana-quickstart
  elasticsearchRefs:
  - name: elasticsearch-quickstart
  mode: fleet
  fleetServerEnabled: true
  policyID: eck-fleet-server
  deployment:
    replicas: 1
    podTemplate:
      spec:
        serviceAccountName: elastic-agent
        automountServiceAccountToken: true
        securityContext:
          runAsUser: 0 <1>
---
apiVersion: agent.k8s.elastic.co/v1alpha1
kind: Agent
metadata:
  name: elastic-agent-quickstart
  namespace: default
spec:
  version: {version}
  kibanaRef:
    name: kibana-quickstart
  fleetServerRef:
    name: fleet-server-quickstart
  mode: fleet
  policyID: eck-agent
  daemonSet:
    podTemplate:
      spec:
        serviceAccountName: elastic-agent
        automountServiceAccountToken: true
        securityContext:
          runAsUser: 0 <1>
        volumes:
        - name: agent-data
          emptyDir: {}
---
apiVersion: kibana.k8s.elastic.co/v1
kind: Kibana
metadata:
  name: kibana-quickstart
  namespace: default
spec:
  version: {version}
  count: 1
  elasticsearchRef:
    name: elasticsearch-quickstart
  config:
    xpack.fleet.agents.elasticsearch.hosts: ["https://elasticsearch-quickstart-es-http.default.svc:9200"]
    xpack.fleet.agents.fleet_server.hosts: ["https://fleet-server-quickstart-agent-http.default.svc:8220"]
    xpack.fleet.packages:
      - name: system
        version: latest
      - name: elastic_agent
        version: latest
      - name: fleet_server
        version: latest
    xpack.fleet.agentPolicies:
      - name: Fleet Server on ECK policy
        id: eck-fleet-server
        namespace: default
        is_managed: true
        monitoring_enabled:
          - logs
          - metrics
        unenroll_timeout: 900
        package_policies:
        - name: fleet_server-1
          id: fleet_server-1
          package:
            name: fleet_server
      - name: Elastic Agent on ECK policy
        id: eck-agent
        namespace: default
        is_managed: true
        monitoring_enabled:
          - logs
          - metrics
        unenroll_timeout: 900
        package_policies:
          - name: system-1
            id: system-1
            package:
              name: system
---
apiVersion: elasticsearch.k8s.elastic.co/v1
kind: Elasticsearch
metadata:
  name: elasticsearch-quickstart
  namespace: default
spec:
  version: {version}
  nodeSets:
  - name: default
    count: 3
    config:
      node.store.allow_mmap: false
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: elastic-agent
rules:
- apiGroups: [""] # "" indicates the core API group
  resources:
  - pods
  - nodes
  - namespaces
  verbs:
  - get
  - watch
  - list
- apiGroups: ["coordination.k8s.io"]
  resources:
  - leases
  verbs:
  - get
  - create
  - update
- apiGroups: ["apps"]
  resources:
  - replicasets
  verbs:
  - list
  - watch
- apiGroups: ["batch"]
  resources:
  - jobs
  verbs:
  - list
  - watch
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: elastic-agent
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: elastic-agent
subjects:
- kind: ServiceAccount
  name: elastic-agent
  namespace: default
roleRef:
  kind: ClusterRole
  name: elastic-agent
  apiGroup: rbac.authorization.k8s.io
EOF
----
+
<1> The root user is required to persist state in a hostPath volume and to trust the {es} CA in {fleet} mode. See <<{p}_storing_local_state_in_host_path_volume>> for options to not run the Agent container as root.
+
Check <<{p}-elastic-agent-fleet-configuration-examples>> for more ready-to-use manifests.

ECK automatically configures secure connections between all components. {fleet} will be set up, and all agents are enrolled in the default policy.

. Monitor the status of {fleet-server} and {agent}.

+
[source,sh]
----
kubectl get agent
----
+
[source,sh,subs="attributes"]
----
NAME                       HEALTH   AVAILABLE   EXPECTED   VERSION      AGE
elastic-agent-quickstart   green    3           3          {version}    14s
fleet-server-quickstart    green    1           1          {version}    19s

----

. List all the Pods belonging to a given {agent} specification.
+
[source,sh]
----
kubectl get pods --selector='agent.k8s.elastic.co/name=elastic-agent-quickstart'
----
+
[source,sh]
----
NAME                                   READY   STATUS    RESTARTS   AGE
elastic-agent-quickstart-agent-t49fd   1/1     Running   0          54s
elastic-agent-quickstart-agent-xbcxr   1/1     Running   0          54s
elastic-agent-quickstart-agent-zqp55   1/1     Running   0          54s
----

. Access logs for one of the Pods.
+
[source,sh]
----
kubectl logs -f elastic-agent-quickstart-agent-xbcxr
----

. Configure the policy used by {agents}. Check link:{fleet-guide}/agent-policy.html[{agent} policies] for more details.

[id="{p}-elastic-agent-fleet-configuration"]
== Configuration

{fleet}-managed {agents} must connect to {fleet-server} to receive their configurations. You can deploy {fleet-server} instances using ECKs Agent CRD with the appropriate configuration, as shown in <<{p}-elastic-agent-fleet-configuration-fleet-mode-and-fleet-server,Fleet mode and Fleet Server>>.

To know more about {fleet} architecture and related components, check the {fleet} link:{fleet-guide}/fleet-server.html[documentation].

[id="{p}-elastic-agent-fleet-configuration-fleet-mode-and-fleet-server"]
=== {fleet} mode and {fleet-server}
To run both {fleet-server} and {agent} in {fleet}-managed mode, set the `mode` configuration element to `fleet`.

[source,yaml,subs="attributes,+macros"]
----
apiVersion: agent.k8s.elastic.co/v1alpha1
kind: Agent
metadata:
  name: elastic-agent-sample
spec:
  mode: fleet
----

To run {fleet-server}, set the `fleetServerEnabled` configuration element to `true`, as shown in this example: 

[source,yaml,subs="attributes,+macros"]
----
apiVersion: agent.k8s.elastic.co/v1alpha1
kind: Agent
metadata:
  name: fleet-server-sample
spec:
  mode: fleet
  fleetServerEnabled: true
----
You can leave the default value `false` for any other case.

[id="{p}-elastic-agent-fleet-configuration-required-kibana-configuration"]
=== Configure {kib}

To have {fleet} running properly, the following settings must be correctly set in the {kib} configuration:

[source,yaml,subs="attributes,+macros"]
----
apiVersion: kibana.k8s.elastic.co/v1
kind: Kibana
metadata:
  name: kibana-sample
spec:
  config:
    xpack.fleet.agents.elasticsearch.hosts: ["https://elasticsearch-sample-es-http.default.svc:9200"]
    xpack.fleet.agents.fleet_server.hosts: ["https://fleet-server-sample-agent-http.default.svc:8220"]
    xpack.fleet.packages:
      - name: system
        version: latest
      - name: elastic_agent
        version: latest
      - name: fleet_server
        version: latest
    xpack.fleet.agentPolicies:
      - name: Fleet Server on ECK policy
        id: eck-fleet-server
        namespace: default
        is_managed: true
        monitoring_enabled:
          - logs
          - metrics
        unenroll_timeout: 900
        package_policies:
        - name: fleet_server-1
          id: fleet_server-1
          package:
            name: fleet_server
      - name: Elastic Agent on ECK policy
        id: eck-agent
        namespace: default
        is_managed: true
        monitoring_enabled:
          - logs
          - metrics
        unenroll_timeout: 900
        is_default: true
        package_policies:
          - name: system-1
            id: system-1
            package:
              name: system
----

*  `xpack.fleet.agents.elasticsearch.hosts` must point to the {es} cluster where {agents} should send data. For ECK-managed {es} clusters ECK creates a Service accessible through `https://ES_RESOURCE_NAME-es-http.ES_RESOURCE_NAMESPACE.svc:9200` URL, where `ES_RESOURCE_NAME` is the name of {es} resource and `ES_RESOURCE_NAMESPACE` is the namespace it was deployed within. See <<{p}_storing_local_state_in_host_path_volume>> for details on adjusting this field when running agent as non-root as it becomes required.

*  `xpack.fleet.agents.fleet_server.hosts` must point to {fleet-server} that {agents} should connect to. For ECK-managed {fleet-server} instances, ECK creates a Service accessible through `https://FS_RESOURCE_NAME-agent-http.FS_RESOURCE_NAMESPACE.svc:8220` URL, where `FS_RESOURCE_NAME` is the name of {agent} resource with {fleet-server} enabled and `FS_RESOURCE_NAMESPACE` is the namespace it was deployed in.

*  `xpack.fleet.packages` are required packages to enable {fleet-server} and {agents} to enroll. 

*  `xpack.fleet.agentPolicies` policies are needed for {fleet-server} and {agents} to enroll to, check {fleet-guide}/agent-policy.html for more information.

[id="{p}-elastic-agent-fleet-configuration-setting-referenced-resources"]
=== Set referenced resources

Both {fleet-server} and {agent} in {fleet} mode can be automatically set up with {fleet} by ECK. The ECK operator can set up {fleet} in {kib} (which otherwise requires manual steps) and enroll {fleet-server} in the default {fleet-server} policy. {agent} can be automatically enrolled in the default {agent} policy. To allow ECK to set this up, provide a reference to a ECK-managed {kib} through the `kibanaRef` configuration element.

[source,yaml,subs="attributes,+macros"]
----
apiVersion: agent.k8s.elastic.co/v1alpha1
kind: Agent
metadata:
  name: fleet-server-sample
spec:
  kibanaRef:
    name: kibana
----

ECK can also facilitate the connection between {agents} and a ECK-managed {fleet-server}. To allow ECK to set this up, provide a reference to {fleet-server} through the `fleetServerRef` configuration element.

[source,yaml,subs="attributes,+macros"]
----
apiVersion: agent.k8s.elastic.co/v1alpha1
kind: Agent
metadata:
  name: elastic-agent-sample
spec:
  fleetServerRef:
    name: fleet-server-sample
----


Set the `elasticsearchRefs` element in your {fleet-server} to point to the {es} cluster that will manage {fleet}. Leave `elasticsearchRefs` empty or unset it for any {agent} running in {fleet} mode as the {es} cluster to target will come from {kib}'s `xpack.fleet.agents.elasticsearch.hosts` configuration element.

NOTE: Currently, {agent} in {fleet} mode supports only a single output, so only a single {es} cluster can be referenced.

[source,yaml,subs="attributes,+macros"]
----
apiVersion: agent.k8s.elastic.co/v1alpha1
kind: Agent
metadata:
  name: fleet-server-sample
spec:
  elasticsearchRefs:
  - name: elasticsearch-sample
----

By default, every reference targets all instances in your {es}, {kib} and {fleet-server} deployments, respectively. If you want to direct traffic to specific instances, refer to <<{p}-traffic-splitting>> for more information and examples.

[id="{p}-elastic-agent-fleet-configuration-custom-configuration"]
=== Customize {agent} configuration

In contrast to {agents} in standalone mode, the configuration is managed through {fleet}, and it cannot be defined through `config` or `configRef` elements.

[id="{p}-elastic-agent-fleet-configuration-upgrade-specification"]
=== Upgrade the {agent} specification

You can upgrade the {agent} version or change settings by editing the YAML specification file. ECK applies the changes by performing a rolling restart of the Agent's Pods. Depending on the settings that you used, ECK will set up {fleet} in {kib}, enrolls the agent in {fleet}, or restarts {agent} on certificate rollover.

[id="{p}-elastic-agent-fleet-configuration-chose-the-deployment-model"]
=== Choose the deployment model

Depending on the use case, {agent} may need to be deployed as a link:https://kubernetes.io/docs/concepts/workloads/controllers/deployment/[Deployment], a link:https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/[DaemonSet], or a link:https://kubernetes.io/docs/concepts/workloads/controllers/statefulSet/[StatefulSet]. To choose how to deploy your {agents}, provide a `podTemplate` element under the `deployment` or the `daemonSet` element in the specification. If you choose the `deployment` option, you can additionally specify the link:https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy[strategy] used to replace old Pods with new ones.

Similarly, you can set the link:https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/[update strategy] when deploying as a DaemonSet. This allows you to control the rollout speed for new configuration by modifying the `maxUnavailable` setting:

[source,yaml,subs="attributes,+macros"]
----
apiVersion: agent.k8s.elastic.co/v1alpha1
kind: Agent
metadata:
  name: elastic-agent-sample
spec:
  version: {version}
  daemonSet:
    strategy:
      type: RollingUpdate
      rollingUpdate:
        maxUnavailable: 3
...
----

Refer to <<{p}-compute-resources-beats-agent>> for more information on how to use the Pod template to adjust the resources given to {agent}.

[id="{p}-elastic-agent-fleet-configuration-role-based-access-control"]
=== Role Based Access Control for {agent}

Some {agent} features, such as the link:https://epr.elastic.co/package/kubernetes/0.2.8/[{k8s} integration], require that Agent Pods interact with {k8s} APIs. This functionality requires specific permissions. Standard {k8s} link:https://kubernetes.io/docs/reference/access-authn-authz/rbac/[RBAC] rules apply. For example, to allow API interactions:

[source,yaml,subs="attributes,+macros"]
----
apiVersion: agent.k8s.elastic.co/v1alpha1
kind: Agent
metadata:
  name: elastic-agent-sample
spec:
  version: {version}
  elasticsearchRefs:
  - name: elasticsearch-sample
  daemonSet:
    podTemplate:
      spec:
        automountServiceAccountToken: true
        serviceAccountName: elastic-agent
...
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: elastic-agent
rules:
- apiGroups: [""] # "" indicates the core API group
  resources:
  - namespaces
  - pods
  - nodes
  - nodes/metrics
  - nodes/proxy
  - nodes/stats
  - events
  verbs:
  - get
  - watch
  - list
- nonResourceURLs:
  - /metrics
  verbs:
  - get
  - watch
  - list
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: elastic-agent
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: elastic-agent
subjects:
- kind: ServiceAccount
  name: elastic-agent
  namespace: default
roleRef:
  kind: ClusterRole
  name: elastic-agent
  apiGroup: rbac.authorization.k8s.io
----

[id="{p}-elastic-agent-fleet-configuration-deploying-in-secured-clusters"]
=== Deploy {agent} in secured clusters

To deploy {agent} in clusters with the Pod Security Policy admission controller enabled, or in <<{p}-openshift-agent,OpenShift>> clusters, you might need to grant additional permissions to the Service Account used by the {agent} Pods. Those Service Accounts must be bound to a Role or ClusterRole that has `use` permission for the required Pod Security Policy or Security Context Constraints. Different {agent} {integrations} might require different settings set in their PSP/link:{p}-openshift-agent.html[SCC].

[id="{p}-elastic-agent-fleet-configuration-customize-fleet-server-service"]
=== Customize {fleet-server} Service

By default, ECK creates a Service for {fleet-server} that {agents} can connect through. You can customize it using the `http` configuration element. Check more information on how to link:k8s-services.html[make changes] to the Service and link:k8s-tls-certificates.html[customize] the TLS configuration.

[id="{p}-elastic-agent-control-fleet-policy-selection"]
=== Control {fleet} policy selection

ECK uses the default policy to enroll {agents} in {fleet} and the default {fleet-server} policy to enroll {fleet-server}. A different policy can be chosen by using the `policyID` attribute in the {agent} resource:
[source,yaml]
----

apiVersion: agent.k8s.elastic.co/v1alpha1
kind: Agent
metadata:
  name: fleet-server-sample
spec:
  policyID: my-custom-policy
...
----

Please note that the environment variables related to policy selection mentioned in the {agent} link:{fleet-guide}/agent-environment-variables.html[docs] like `FLEET_SERVER_POLICY_ID` will be managed by the ECK operator.


[id="{p}-elastic-agent-running-as-a-non-root-user"]
// tag::configuration-example-elastic-agent-running-as-a-non-root-user[]
=== Running as a non-root user

In order to run {agent} as a non-root user you must choose how you want to persist data to the Agent's volume.

1. Run {agent} with an `emptyDir` volume. This has the downside of not persisting data between restarts of the {agent} which can duplicate work done by the previous running Agent.
2. Run {agent} with a `hostPath` volume in addition to a `DaemonSet` running as `root` that sets up permissions for the `agent` user.

In addition to these decisions, if you are running {agent} in {fleet} mode as a non-root user, you must configure `certificate_authorities.ssl` in each `xpack.fleet.outputs` to trust the CA of the {es} Cluster.

To run {agent} with an `emptyDir` volume.

[source,yaml]
----
apiVersion: agent.k8s.elastic.co/v1alpha1
kind: Agent
metadata:
  name: fleet-server
spec:
  deployment:
    podTemplate:
      spec:
        securityContext: <1>
          fsGroup: 1000
        volumes:
        - name: agent-data
          emptyDir: {}
...
----
<1> Gid 1000 is the default group at which the Agent container runs. Adjust as necessary if `runAsGroup` has been modified.

To run {agent} with a `hostPath` volume and a `DaemonSet` to maintain permissions.

[source,yaml]
----
apiVersion: agent.k8s.elastic.co/v1alpha1
kind: Agent
metadata:
  name: fleet-server-sample
  namespace: elastic-apps
spec:
  mode: fleet
  fleetServerEnabled: true
  deployment: {}
...
---
apiVersion: agent.k8s.elastic.co/v1alpha1
kind: Agent
metadata:
  name: elastic-agent-sample
  namespace: elastic-apps
spec:
  daemonSet: {}
...
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: manage-agent-hostpath-permissions
  namespace: elastic-apps
spec:
  selector:
    matchLabels:
      name: manage-agent-hostpath-permissions
  template:
    metadata:
      labels:
        name: manage-agent-hostpath-permissions
    spec:
      # serviceAccountName: elastic-agent <1>
      volumes:
        - hostPath:
            path: /var/lib/elastic-agent
            type: DirectoryOrCreate
          name: "agent-data"
      initContainers:
        - name: manage-agent-hostpath-permissions
          # image: registry.access.redhat.com/ubi9/ubi-minimal:latest <2>
          image: docker.io/bash:5.2.15
          resources:
            limits:
              cpu: 100m
              memory: 32Mi
          securityContext:
            # privileged: true <3>
            runAsUser: 0
          volumeMounts:
            - mountPath: /var/lib/elastic-agent
              name: agent-data
          command:
          - 'bash'
          - '-e'
          - '-c'
          - |-
            # Adjust this with /var/lib/elastic-agent/YOUR-NAMESPACE/YOUR-AGENT-NAME/state
            # Multiple directories are supported for the fleet-server + agent use case.
            dirs=(
              "/var/lib/elastic-agent/default/elastic-agent/state"
              "/var/lib/elastic-agent/default/fleet-server/state"
              )
            for dir in ${dirs[@]}; do
              mkdir -p "${dir}"
              # chcon is only required when running an an SELinux-enabled/OpenShift environment.
              # chcon -Rt svirt_sandbox_file_t "${dir}"
              chmod g+rw "${dir}"
              # Gid 1000 is the default group at which the Agent container runs. Adjust as necessary if `runAsGroup` has been modified.
              chgrp 1000 "${dir}"
              if [ -n "$(ls -A ${dir} 2>/dev/null)" ]
              then
                # Gid 1000 is the default group at which the Agent container runs. Adjust as necessary if `runAsGroup` has been modified.
                chgrp 1000 "${dir}"/*
                chmod g+rw "${dir}"/*
              fi
            done
      containers:
        - name: sleep
          image: gcr.io/google-containers/pause-amd64:3.2
----
<1> This is only required when running in an SElinux-enabled/OpenShift environment. Ensure this user has been added to the privileged security context constraints (SCC) in the correct namespace. `oc adm policy add-scc-to-user privileged -z elastic-agent -n elastic-apps`
<2> UBI is only required when needing the `chcon` binary when running in an SELinux-enabled/OpenShift environment. If that is not required then the following smaller image can be used instead: `docker.io/bash:5.2.15`
<3> Privileged is only required when running in an SElinux-enabled/OpenShift environment.

When running Agent in fleet mode as a non-root user {kib} must be configured in order to properly accept the CA of the {es} cluster.

[source,yaml]
----
---
apiVersion: kibana.k8s.elastic.co/v1
kind: Kibana
metadata:
  name: kibana-sample
spec:
  config:
    # xpack.fleet.agents.elasticsearch.hosts: <1>
    xpack.fleet.agents.fleet_server.hosts: ["https://fleet-server-sample-agent-http.default.svc:8220"]
    xpack.fleet.outputs:
    - id: eck-fleet-agent-output-elasticsearch
      is_default: true
      name: eck-elasticsearch
      type: elasticsearch
      hosts:
      - "https://elasticsearch-sample-es-http.default.svc:9200" <2>
      ssl:
        certificate_authorities: ["/mnt/elastic-internal/elasticsearch-association/default/elasticsearch-sample/certs/ca.crt"] <3>
----

<1> This entry must not exist when running agent in fleet mode as a non-root user.
<2> Note that the correct URL for {es} is `https://ELASTICSEARCH_NAME-es-http.YOUR-NAMESPACE.svc:9200`
<3> Note that the correct path for {es} `certificate_authorities` is `/mnt/elastic-internal/elasticsearch-association/YOUR-NAMESPACE/ELASTICSEARCH-NAME/certs/ca.crt`

// end::configuration-example-elastic-agent-running-as-a-non-root-user[]

[id="{p}-elastic-agent-fleet-configuration-examples"]
== Configuration Examples

This section contains manifests that illustrate common use cases, and can be your starting point in exploring {agent} deployed with ECK. These manifests are self-contained and work out-of-the-box on any non-secured {k8s} cluster. They all contain a three-node {es} cluster, a single {kib} instance and a single {fleet-server} instance.

CAUTION: The examples in this section are for illustration purposes only and should not be considered to be production-ready. Some of these examples use the `node.store.allow_mmap: false` setting which has performance implications and should be tuned for production workloads, as described in <<{p}-virtual-memory>>.


=== System and {k8s} {integrations}

[source,sh,subs="attributes"]
----
kubectl apply -f {agent_recipes}/fleet-kubernetes-integration.yaml
----
Deploys {agent} as a DaemonSet in {fleet} mode with System and {k8s} {integrations} enabled. System integration collects syslog logs, auth logs and system metrics (for CPU, I/O, filesystem, memory, network, process and others). {k8s} {integrations} collects API server, Container, Event, Node, Pod, Volume and system metrics.

=== System and {k8s} {integrations} running as non-root

[source,sh,subs="attributes"]
----
kubectl apply -f {agent_recipes}/fleet-kubernetes-integration-nonroot.yaml
----
The provided example is functionally identical to the previous section but runs the {agent} processes (both the {agent} running as the {fleet} server and the {agent} connected to {fleet}) as a non-root user by utilizing a DaemonSet to ensure directory and file permissions.

NOTE: The DaemonSet itself must run as root to set up permissions and ECK >= 2.10.0 is required.

=== Custom logs integration with autodiscover

[source,sh,subs="attributes"]
----
kubectl apply -f {agent_recipes}/fleet-custom-logs-integration.yaml
----

Deploys {agent} as a DaemonSet in {fleet} mode with Custom Logs integration enabled. Collects logs from all Pods in the `default` namespace using autodiscover feature.


=== APM integration

[source,sh,subs="attributes"]
----
kubectl apply -f {agent_recipes}/fleet-apm-integration.yaml
----

Deploys single instance {agent} Deployment in {fleet} mode with APM integration enabled.

=== Synthetic monitoring

[source,sh,subs="attributes"]
----
kubectl apply -f {agent_recipes}/synthetic-monitoring.yaml
----

Deploys an {fleet}-enrolled {agent} that can be used as for link:{observability-guide}/monitor-uptime-synthetics.html[Synthetic monitoring]. This {agent} uses the `elastic-agent-complete` image. The agent policy still needs to be link:{observability-guide}/synthetics-private-location.html#synthetics-private-location-add[registered as private location] in {kib}.

[id="{p}-elastic-agent-fleet-known-limitations"]
== Known limitations

=== Running as root (ECK < 2.10.0 and Agent < 7.14.0)
Until version 7.14.0 and ECK version 2.10.0, {agent} and {fleet-server} were required to run as root.

As of {stack} version 7.14.0 and ECK version 2.10.0 it is also possible to run {agent} and {fleet} as a non-root user. See <<{p}_storing_local_state_in_host_path_volume>> for instructions.

=== {agent} running in the same namespace as the {stack}.
Until ECK version 2.11.0, {agent} and {fleet-server} were required to run within the same Namespace as {es}.

As of ECK version 2.11.0, {agent}, {fleet-server} and {es} can all be deployed in different Namespaces.

=== Running {endpoint-sec} integration
Running {endpoint-sec} link:{security-guide}/install-endpoint.html[integration] is not yet supported in containerized environments, like {k8s}. This is not an ECK limitation, but the limitation of the integration itself. Note that you can use ECK to deploy {es}, {kib} and {fleet-server}, and add {endpoint-sec} integration to your policies if {agents} running those policies are deployed in non-containerized environments.

=== {fleet-server} initialization fails on minikube when CNI is disabled
When deployed with ECK, the {fleet-server} Pod makes an HTTP call to itself during {fleet} initialization using its Service. Since a link:https://github.com/kubernetes/minikube/issues/1568[Pod cannot reach itself through its Service on minikube] when CNI is disabled, the call hangs until the connection times out and the Pod enters a crash loop.

Solution: enable CNI when starting minikube: `minikube start --cni=true`.

// tag::elastic-agent-fleet-known-limitations-local-state[]

=== Storing local state in host path volume
{agent} managed by ECK stores local state in a host path volume by default. This ensures that {integrations} run by the agent can continue their work without duplicating work that has already been done after the Pod has been recreated for example because of a Pod configuration change. Multiple replicas of an agent, for example {fleet} Servers, can not be deployed on the same underlying {k8s} node as they would try to use the same host path. There are 2 options for managing this feature:

1. If local state storage in `hostPath` volumes is not desired this can be turned off by configuring an `emptyDir` volume instead.
2. If local state storage is still desired but running the Agent container as root is not allowed, then you can run a `DaemonSet` that adjusts the permissions for the Agent local state on each Node prior to running {agent}. Note that this `DaemonSet` must be `runAsUser: 0` and possibly `privileged: true`. Also note the {kib} changes required to trust the {es} CA when running in fleet mode.

Full configuration examples exist in  <<{p}-elastic-agent-running-as-a-non-root-user>>.

// end::elastic-agent-fleet-known-limitations-local-state[]
