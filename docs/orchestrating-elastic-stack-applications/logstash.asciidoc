:page_id: logstash
:logstash_recipes: https://raw.githubusercontent.com/elastic/cloud-on-k8s/{eck_release_branch}/config/recipes/logstash
ifdef::env-github[]
****
link:https://www.elastic.co/guide/en/cloud-on-k8s/master/k8s-{page_id}.html[View this document on the Elastic website]
****
endif::[]
[id="{p}-{page_id}"]
= Run {ls} on ECK

This section describes how to configure and deploy {ls} with ECK.

* <<{p}-logstash-quickstart>>
* <<{p}-logstash-configuration>>
** <<{p}-logstash-configuring-logstash>>
** <<{p}-logstash-pipelines>>
** <<{p}-logstash-volumes>>
** <<{p}-logstash-pipelines-es>>
** <<{p}-logstash-expose-services>>
* <<{p}-logstash-securing-api>>
* <<{p}-logstash-plugins>>
** <<{p}-plugin-resources>>
** <<{p}-logstash-working-with-plugins-scaling>>
** <<{p}-logstash-working-with-plugin-considerations>>
** <<{p}-logstash-working-with-custom-plugins>>
* <<{p}-logstash-configuration-examples>>
* <<{p}-logstash-update-strategy>>
* <<{p}-logstash-advanced-configuration>>
** <<{p}-logstash-jvm-options>>
** <<{p}-logstash-keystore>> 


NOTE: Running {ls} on ECK is compatible only with {ls} 8.7+.

[id="{p}-logstash-quickstart"]
== Quickstart

Add the following specification to create a minimal {ls} deployment that will listen to a Beats agent or Elastic Agent configured to send to Logstash on port 5044, create the service and write the output to an Elasticsearch cluster named `quickstart`, created in the link:k8s-quickstart.html[Elasticsearch quickstart].

[source,yaml,subs="attributes,+macros,callouts"]
----
cat $$<<$$'EOF' | kubectl apply -f -
apiVersion: logstash.k8s.elastic.co/v1alpha1
kind: Logstash
metadata:
  name: quickstart
spec:
  count: 1
  elasticsearchRefs:
    - name: quickstart
      clusterName: qs
  version: {version}
  pipelines:
    - pipeline.id: main
      config.string: |
        input {
          beats {
            port => 5044
          }
        }
        output {
          elasticsearch {
            hosts => [ "${QS_ES_HOSTS}" ]
            user => "${QS_ES_USER}"
            password => "${QS_ES_PASSWORD}"
            ssl_certificate_authorities => "${QS_ES_SSL_CERTIFICATE_AUTHORITY}"
          }
        }
  services:
    - name: beats
      service:
        spec:
          type: NodePort
          ports:
            - port: 5044
              name: "filebeat"
              protocol: TCP
              targetPort: 5044
EOF
----

Check <<{p}-logstash-configuration-examples>> for more ready-to-use manifests.

. Check the status of Logstash
+
[source,sh]
----
kubectl get logstash
----
+
[source,sh,subs="attributes"]
----
NAME              AVAILABLE   EXPECTED   AGE   VERSION
quickstart        3           3          4s    {version}
----

. List all the Pods that belong to a given Logstash specification.
+
[source,sh]
----
kubectl get pods --selector='logstash.k8s.elastic.co/name=quickstart'
----
+
[source,sh]
----
NAME              READY   STATUS    RESTARTS   AGE
quickstart-ls-0   1/1     Running   0          91s
----

. Access logs for a Pod.

[source,sh]
----
kubectl logs -f quickstart-ls-0
----

[id="{p}-logstash-configuration"]
== Configuration

[id="{p}-logstash-upgrade-specification"]
=== Upgrade the Logstash specification

You can upgrade the Logstash version or change settings by editing the YAML specification. ECK applies the changes by performing a rolling restart of Logstash Pods.

[id="{p}-logstash-configuring-logstash"]
=== Logstash configuration

Define the Logstash configuration (the ECK equivalent to `logstash.yml`) in the `spec.config` section:

[source,yaml,subs="attributes,+macros,callouts"]
----
apiVersion: logstash.k8s.elastic.co/v1alpha1
kind: Logstash
metadata:
  name: quickstart
spec:
  version: {version}
  count: 1
  elasticsearchRefs:
  - name: quickstart
    clusterName: qs
  config: <1>
    pipeline.workers: 4  
    log.level: debug
----
<1> Customize Logstash configuration using `logstash.yml` settings here


Alternatively, you can provide the configuration through a Secret specified in the `spec.configRef` section. The Secret must have a `logstash.yml` entry with your settings:
[source,yaml,subs="attributes,+macros"]
----
apiVersion: logstash.k8s.elastic.co/v1alpha1
kind: Logstash
metadata:
  name: quickstart
spec:
  version: {version}
  count: 1
  elasticsearchRefs:
  - name: quickstart
    clusterName: qs
  configRef:
    secretName: quickstart-config
---
apiVersion: v1
kind: Secret
metadata:
  name: quickstart-config
stringData:
  logstash.yml: |-
    pipeline.workers: 4
    log.level: debug
----


[id="{p}-logstash-pipelines"]
=== Configuring Logstash pipelines

Define Logstash pipelines in the `spec.pipelines` section (the ECK equivalent to `pipelines.yml`):

[source,yaml,subs="attributes,+macros,callouts"]
----
apiVersion: logstash.k8s.elastic.co/v1alpha1
kind: Logstash
metadata:
  name: quickstart
spec:
  version: {version}
  count: 1
  elasticsearchRefs:
    - clusterName: qs
      name: quickstart
  pipelines:
    - pipeline.id: main
      config.string: |
        input {
          beats {
            port => 5044
          }
        }
        output {
          elasticsearch {
            hosts => [ "${QS_ES_HOSTS}" ]
            user => "${QS_ES_USER}"
            password => "${QS_ES_PASSWORD}"
            ssl_certificate_authorities => "${QS_ES_SSL_CERTIFICATE_AUTHORITY}"
          }
        }
----

Alternatively, you can provide the pipelines configuration through a Secret specified in the `spec.pipelinesRef` field. The Secret must have a `pipelines.yml` entry with your configuration:
[source,yaml,subs="attributes,+macros"]
----
apiVersion: logstash.k8s.elastic.co/v1alpha1
kind: Logstash
metadata:
  name: quickstart
spec:
  version: {version}
  count: 1
  elasticsearchRefs:
    - clusterName: qs
      name: quickstart
  pipelinesRef:
    secretName: quickstart-pipeline
---
apiVersion: v1
kind: Secret
metadata:
  name: quickstart-pipeline
stringData:
  pipelines.yml: |-
    - pipeline.id: main
      config.string: |
        input {
          beats {
            port => 5044
          }
        }
        output {
          elasticsearch {
            hosts => [ "${QS_ES_HOSTS}" ]
            user => "${QS_ES_USER}"
            password => "${QS_ES_PASSWORD}"
            ssl_certificate_authorities => "${QS_ES_SSL_CERTIFICATE_AUTHORITY}"
          }
        }

----

Logstash on ECK supports all options present in `pipelines.yml`, including settings to update the number of workers, and
 the size of the batch that the pipeline will process. This also includes using `path.config` to point to volumes
 mounted on the Logstash container:

[source,yaml,subs="attributes,+macros,callouts"]
----
apiVersion: logstash.k8s.elastic.co/v1alpha1
kind: Logstash
metadata:
  name: quickstart
spec:
  version: {version}
  count: 1
  elasticsearchRefs:
    - clusterName: qs
      name: quickstart
  pipelines:
    - pipeline.id: main
      config.string: |
        input {
          beats {
            port => 5044
          }
        }
        output {
          elasticsearch {
            hosts => [ "${QS_ES_HOSTS}" ]
            user => "${QS_ES_USER}"
            password => "${QS_ES_PASSWORD}"
            ssl_certificate_authorities => "${QS_ES_SSL_CERTIFICATE_AUTHORITY}"
          }
        }
----

NOTE: Logstash persistent queues (PQs) and dead letter queues (DLQs) are not currently managed by the Logstash operator, and using them will require you to create and manage your own Volumes and VolumeMounts

[id="{p}-logstash-volumes"]
=== Defining data volumes for Logstash
added:[2.9.0]

WARNING: Volume support for Logstash is a breaking change to earlier versions of ECK and requires you to recreate your Logstash resources.


[id="{p}-volume-claim-settings"]
[discrete]
== Specifying the volume claim settings

A PersistentVolume called `logstash-data` is created by default.
It maps to `/usr/share/logstash/data` for persistent storage, which is typically used for storage from plugins. 

By default, the `logstash-data` volume claim is a `1.5Gi` volume, using the standard StorageClass of your Kubernetes cluster. 
You can override the default by adding a `spec.volumeClaimTemplate` section named `logstash-data`.

For production workloads, you should define your own volume claim template with the desired storage capacity and (optionally) the Kubernetes link:https://kubernetes.io/docs/concepts/storage/storage-classes/[storage class] to associate with the persistent volume. To override this volume claim for `data` usages, the name of this volume claim must be `logstash-data`.

This example updates the default data template to increase the storage to `2Gi` for the {ls} data folder:

[source,yaml,subs="attributes,+macros,callouts"]
----
apiVersion: logstash.k8s.elastic.co/v1alpha1
kind: Logstash
metadata:
  name: logstash
spec:
  # some configuration attributes omitted for brevity here
  volumeClaimTemplates:
    - metadata:
        name: logstash-data # Do not change this name unless you set up a volume mount for the data path.
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 2Gi
----

The default volume size will likely be insufficient for production workloads, especially when you are using:

* the persistent queue (PQ) feature
* dead letter queues (DLQ), or 
* {ls} plugins that make heavy use of temporary storage. 

Increase the storage capacity, or consider creating separate volumes for these use cases.

You can add separate storage by including an additional `spec.volumeClaimTemplate` along with a corresponding `spec.podTemplate.spec.containers.volumeMount` for each requested volume.

This example shows how to setup separate storage for a PQ:


[source,yaml,subs="attributes,+macros,callouts"]
----
apiVersion: logstash.k8s.elastic.co/v1alpha1
kind: Logstash
metadata:
  name: logstash
spec:
  # some configuration attributes omitted for brevity here
  volumeClaimTemplates:
    - metadata:
        name: pq <1>
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 10Gi
  podTemplate:
    spec:
      containers:
      - name: logstash
        volumeMounts:
        - mountPath: /usr/share/logstash/pq <2>
          name: pq  <1>
          readOnly: false
  config:
    log.level: info
    queue.type: persisted
    path.queue: /usr/share/logstash/pq <2>
----
<1> The `name` values in the `volumeMount` for the container in the `podTemplate` section and the name of the `volumeClaimTemplate` must match.
<2> Set the `path.queue` setting in the configuration to match the `mountPath` in the `volumeMount`.


This example shows how to configure {ls} with a Dead Letter Queue setup on the main pipeline, and a separate pipeline to read items from the DLQ.

[source,yaml,subs="attributes,+macros,callouts"]
----
apiVersion: logstash.k8s.elastic.co/v1alpha1
kind: Logstash
metadata:
  name: logstash
spec:
   # some configuration attributes omitted for brevity here
   podTemplate:
    spec:
      containers:
      - name: logstash
        volumeMounts:
        - mountPath: /usr/share/logstash/dlq <2>
          name: dlq  <1>
          readOnly: false
  volumeClaimTemplates:
    - metadata:
      name: dlq <1>
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 10Gi
  pipelines:
    - pipeline.id: beats
      dead_letter_queue.enable: true
      path.dead_letter_queue: /usr/share/logstash/dlq <2>
      config.string: |
        input {
          beats {
            port => 5044
          }
        }
        output {
          elasticsearch {
            hosts => [ "${ECK_ES_HOSTS}" ]
            user => "${ECK_ES_USER}"
            password => "${ECK_ES_PASSWORD}"
            ssl_certificate_authorities => "${ECK_ES_SSL_CERTIFICATE_AUTHORITY}"
          }
        }
    - pipeline.id: dlq_read
      dead_letter_queue.enable: false
      config.string: |
        input {
          dead_letter_queue {
            path => "/usr/share/logstash/dlq" <2>
            commit_offsets => true
            pipeline_id => "beats"
            clean_consumed => true
          }
        }
        filter {
          mutate {
            remove_field => "[geoip][location]"
          }
        }
        output {
          elasticsearch {
            hosts => [ "${ECK_ES_HOSTS}" ]
            user => "${ECK_ES_USER}"
            password => "${ECK_ES_PASSWORD}"
            ssl_certificate_authorities => "${ECK_ES_SSL_CERTIFICATE_AUTHORITY}"
          }
        }
----
<1> The `name` values in the `volumeMount` for the container in the `podTemplate` section and the name of the `volumeClaimTemplate` must match.
<2> Set the `path.dead_letter_queue` setting in the pipeline config to match the `mountPath` in the `volumeMount` for pipelines that are writing to the Dead Letter Queue, and set the `path` setting of the `dead_letter_queue` plugin for the pipeline that will read from the Dead Letter Queue.


[id="{p}-volume-claim-settings-updates"]
[discrete]
== Updating the volume claim settings

If the storage class allows link:https://kubernetes.io/blog/2018/07/12/resizing-persistent-volumes-using-kubernetes/[volume expansion], you can increase the storage requests size in `spec.volumeClaimTemplates`. 
ECK updates the existing PersistentVolumeClaims accordingly, and recreates the StatefulSet automatically. 

If the volume driver supports `ExpandInUsePersistentVolumes`, the filesystem is resized online.
In this case, you do not need to restart the {ls} process or re-create the Pods. 

If the volume driver does not support `ExpandInUsePersistentVolumes`, you must manually delete Pods after the resize so that they can be recreated automatically with the expanded filesystem.

Any other changes in the volumeClaimTemplates--such as changing the storage class or decreasing the volume size--are not allowed. 
To make changes such as these, you must fully delete the {ls} resource, delete and recreate or resize the volume, and create a new {ls} resource.

Before you delete a persistent queue (PQ) volume, ensure that the queue is empty.
We recommend setting `queue.drain: true` on the {ls} Pods to ensure that the queue is drained when Pods are shutdown.
Note that you should also increase the `terminationGracePeriodSeconds` to a large enough value to allow the queue to drain.

This example shows how to configure a {ls} resource to drain the queue and increase the termination grace period.

[source,yaml,subs="attributes,+macros,callouts"]
----
apiVersion: logstash.k8s.elastic.co/v1alpha1
kind: Logstash
metadata:
  name: logstash
spec:
  # some configuration attributes omitted for brevity here
  config:
    queue.drain: true
  podTemplate:
    spec:
      terminationGracePeriodSeconds: 604800
----

NOTE: A https://github.com/kubernetes/kubernetes/issues/94435[{k8s} known issue]: {k8s} may not honor `terminationGracePeriodSeconds` settings greater than 600.
A queue of a terminated Pod may not be fully drained, even when `queue.drain: true` is set and a high `terminationGracePeriodSeconds` is configured.

NOTE: In this technical preview, there is currently no way to drain a dead letter queue (DLQ) automatically before {ls} shuts down.
To manually drain the queue, first stop sending data to it, by either disabling the DLQ feature, or disabling any pipelines that send to a DLQ.
Then wait for events to stop flowing through any pipelines reading from the input.


[id="{p}-emptydir"]
[discrete]
== EmptyDir

If you are not concerned about data loss, you can use an `emptyDir` volume for Logstash data.

[CAUTION]
--
The use of `emptyDir` in a production environment may cause permanent data loss.
Do not use with persistent queues (PQs), dead letter queues (DLQs), or with any plugin that requires persistent storage to keep track of state between restarts of {ls}.

Plugins that require persistent storage include any plugin that stores state locally.
These plugins typically have a configuration parameter that includes the name `path` or `directory`, not including paths to static content, such as certificates or keystores.
Examples include the `sincedb_path` setting for the `file`, `dead_letter_queue` and `s3` inputs, the `last_run_metadata_path` for the `JDBC` input, `aggregate_maps_path` for the `aggregate` filter, and `temporary_directory` for the `s3` output, used to aggregate content before uploading to s3.
--


[source,yaml]
----
spec:
  count: 5
  podTemplate:
    spec:
      volumes:
      - name: logstash-data
        emptyDir: {}
----


[id="{p}-logstash-pipelines-es"]
=== Using Elasticsearch in Logstash pipelines

[id="{p}-logstash-esref"]
==== `elasticsearchRefs` for establishing a secured connection

The `spec.elasticsearchRefs` section provides a mechanism to help configure Logstash to establish a secured connection to one or more ECK managed Elasticsearch clusters. By default, each `elasticsearchRef` will target all nodes in its referenced Elasticsearch cluster. If you want to direct traffic to specific nodes of your Elasticsearch cluster, refer to <<{p}-traffic-splitting>> for more information and examples.

When you use `elasticsearchRefs` in a Logstash pipeline, the Logstash operator creates the necessary resources from the associated Elasticsearch cluster, and provides environment variables to allow these resources to be accessed from the pipeline configuration. 
Environment variables are replaced at runtime with the appropriate values.
The environment variables have a fixed naming convention:

* `NORMALIZED_CLUSTERNAME_ES_HOSTS`
* `NORMALIZED_CLUSTERNAME_ES_USER`
* `NORMALIZED_CLUSTERNAME_ES_PASSWORD`
* `NORMALIZED_CLUSTERNAME_ES_SSL_CERTIFICATE_AUTHORITY`

where NORMALIZED_CLUSTERNAME is the value taken from the `clusterName` field of the `elasticsearchRef` property, capitalized, with `-` transformed to `_`. That is, `prod-es` would become `PROD_ES`.

[NOTE]
--
* The `clusterName` value should be unique across all referenced {es} instances in the same {ls} spec.
* The {ls} ECK operator creates a user called `eck_logstash_user_role` when an `elasticsearchRef` is specified. This user has the following permissions:
+
```
  "cluster": ["monitor", "manage_ilm", "read_ilm", "manage_logstash_pipelines", "manage_index_templates", "cluster:admin/ingest/pipeline/get",]
  "indices": [
    {
      "names": [ "logstash", "logstash-*", "ecs-logstash", "ecs-logstash-*", "logs-*", "metrics-*", "synthetics-*", "traces-*" ],
      "privileges": ["manage", "write", "create_index", "read", "view_index_metadata"]
    }
]
```
+
You can <<{p}-users-and-roles,update user permissions>> to include more indices if the Elasticsearch plugin is expected to use indices other than the default. Check out <<{p}-logstash-configuration-custom-index, Logstash configuration with a custom index>> sample configuration that creates a user that writes to a custom index.
--

This example demonstrates how to create a Logstash deployment that connects to
different Elasticsearch instances, one of which is in a separate namespace:

[source,yaml,subs="attributes,+macros,callouts"]
----
apiVersion: logstash.k8s.elastic.co/v1alpha1
kind: Logstash
metadata:
  name: quickstart
spec:
  version: {version}
  count: 1
  elasticsearchRefs:        <1>
    - clusterName: prod-es  <2>
      name: prod
    - clusterName: qa-es    <3>
      name: qa
      namespace: qa
  pipelines:
    - pipeline.id: main
      config.string: |
        input {
          beats {
            port => 5044
          }
        }
        output {
          elasticsearch {   <4>
            hosts => [ "${PROD_ES_ES_HOSTS}" ]
            user => "${PROD_ES_ES_USER}"
            password => "${PROD_ES_ES_PASSWORD}"
            ssl_certificate_authorities => "${PROD_ES_ES_SSL_CERTIFICATE_AUTHORITY}"
          }
          elasticsearch {   <4>
            hosts => [ "${QA_ES_ES_HOSTS}" ]
            user => "${QA_ES_ES_USER}"
            password => "${QA_ES_ES_PASSWORD}"
            ssl_certificate_authorities => "${QA_ES_ES_SSL_CERTIFICATE_AUTHORITY}"
          }
        }

----

<1> Define Elasticsearch references in the CRD. This will create the appropriate Secrets to store certificate details and the rest of the connection information, and create environment variables to allow them to be referred to in Logstash pipeline configurations.
<2> This refers to an Elasticsearch cluster residing in the same namespace as the Logstash instances.
<3> This refers to an Elasticsearch cluster residing in a different namespace to the Logstash instances.
<4> Elasticsearch output definitions - use the environment variables created by the Logstash operator when specifying an `ElasticsearchRef`. Note the use of "normalized" versions of the `clusterName` in the environment variables used to populate the relevant fields.


[id="{p}-logstash-external-es"]
==== Connect to an external Elasticsearch cluster

Logstash can connect to external Elasticsearch cluster that is not managed by ECK.
You can reference a Secret instead of an Elasticsearch cluster in the `elasticsearchRefs` section through the `secretName` attribute:

[source,yaml,subs="attributes,callouts"]
----
apiVersion: v1
kind: Secret
metadata:
  name: external-es-ref
stringData:
  url: https://abcd-42.xyz.elastic-cloud.com:443 <1>
  username: logstash_user <2>
  password: REDACTED <3>
  ca.crt: REDACTED <4>
---
apiVersion: logstash.k8s.elastic.co/v1alpha1
kind: Logstash
metadata:
  name: quickstart
spec:
  version: {version}
  count: 1
  elasticsearchRefs:
    - clusterName: prod-es
      secretName: external-es-ref <5>
  monitoring:
    metrics:
      elasticsearchRefs:
      - secretName: external-es-ref <5>
    logs:
      elasticsearchRefs:
      - secretName: external-es-ref <5>
----

<1> The URL to reach the {es} cluster.
<2> The username of the user to be authenticated to the {es} cluster.
<3> The password of the user to be authenticated to the {es} cluster.
<4> The CA certificate in PEM format to secure communication to the {es} cluster (optional).
<5> The `secretName` and `name` attributes are mutually exclusive, you have to choose one or the other.

TIP: Always specify the port in the URL when {ls} is connecting to an external {es} cluster.

[id="{p}-logstash-expose-services"]
=== Expose services

By default, the {ls} operator creates a headless Service for the metrics endpoint to enable metric collection by the Metricbeat sidecar for Stack Monitoring:


[source,sh]
----
kubectl get service quickstart-ls-api
----

[source,sh,subs="attributes"]
----
NAME                TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE
quickstart-ls-api   ClusterIP   None         <none>        9600/TCP   48s
----

Additional services can be added in the `spec.services` section of the resource:

[source,yaml,subs="attributes,+macros,callouts"]
----
services:
  - name: beats
    service:
      spec:
        ports:
        - port: 5044
          name: "winlogbeat"
          protocol: TCP
        - port: 5045
          name: "filebeat"
          protocol: TCP
----

[id="{p}-logstash-pod-configuration"]
=== Pod configuration
You can <<{p}-customize-pods,customize the {ls} Pod>> using a Pod template, defined in the `spec.podTemplate` section of the configuration.

This example demonstrates how to create a {ls} deployment with increased heap size and resource limits.

[source,yaml,subs="attributes"]
----
apiVersion: logstash.k8s.elastic.co/v1alpha1
kind: Logstash
metadata:
  name: logstash-sample
spec:
  version: {version}
  count: 1
  elasticsearchRefs:
    - name: "elasticsearch-sample"
      clusterName: "sample"
  podTemplate:
    spec:
      containers:
      - name: logtash
        env:
        - name: LS_JAVA_OPTS
          value: "-Xmx2g -Xms2g"
        resources:
          requests:
            memory: 1Gi
            cpu: 0.5
          limits:
            memory: 4Gi
            cpu: 2
----

The name of the container in the Pod template must be `logstash`.


[id="{p}-logstash-securing-api"]
== Securing Logstash API

[id="{p}-logstash-https"]
=== Enable HTTPS

Access to the link:https://www.elastic.co/guide/en/logstash/current/monitoring-logstash.html#monitoring-api-security[Logstash Monitoring APIs] use HTTPS by default - the operator will set the values  `api.ssl.enabled: true`, `api.ssl.keystore.path` and `api.ssl.keystore.password`.

You can further secure the {ls} Monitoring APIs by requiring HTTP Basic authentication by setting `api.auth.type: basic`, and providing the relevant credentials `api.auth.basic.username` and `api.auth.basic.password`:

[source,yaml,subs="attributes,+macros,callouts"]
----
apiVersion: v1
kind: Secret
metadata:
  name: logstash-api-secret
stringData:
  API_USERNAME: "AWESOME_USER"   <1>
  API_PASSWORD: "T0p_Secret"     <1>
---
apiVersion: logstash.k8s.elastic.co/v1alpha1
kind: Logstash
metadata:
  name: logstash-sample
spec:
  version: {version}
  count: 1
  config:
    api.auth.type: basic
    api.auth.basic.username: "${API_USERNAME}"   <3>
    api.auth.basic.password: "${API_PASSWORD}"   <3>
  podTemplate:
    spec:
      containers:
        - name: logstash
          envFrom:
            - secretRef:
                name: logstash-api-secret   <2>
----
<1> Store the username and password in a Secret.
<2> Map the username and password to the environment variables of the Pod.
<3> At Logstash startup, `${API_USERNAME}` and `${API_PASSWORD}` are replaced by the value of environment variables. Check link:https://www.elastic.co/guide/en/logstash/current/environment-variables.html[using environment variables] for more details.

An alternative is to set up <<{p}-logstash-keystore, keystore>> to resolve `${API_USERNAME}` and `${API_PASSWORD}`

NOTE: The variable substitution in `config` does not support the default value syntax.

[id="{p}-logstash-http-tls-keystore"]
=== TLS keystore

The TLS Keystore is automatically generated and includes a certificate and a private key, with default password protection set to `changeit`.
This password can be modified by configuring the `api.ssl.keystore.password` value.

[source,yaml,subs="attributes"]
----
apiVersion: logstash.k8s.elastic.co/v1alpha1
kind: Logstash
metadata:
  name: logstash-sample
spec:
  count: 1
  version: {version}
  config:
    api.ssl.keystore.password: "${SSL_KEYSTORE_PASSWORD}"
----


[id="{p}-logstash-http-custom-tls"]
=== Provide your own certificate

If you want to use your own certificate, the required configuration is similar to Elasticsearch. Configure the certificate in `api` Service. Check <<{p}-custom-http-certificate>>.

[source,yaml,subs="attributes,+macros,callouts"]
----
apiVersion: logstash.k8s.elastic.co/v1alpha1
kind: Logstash
metadata:
  name: logstash-sample
spec:
  version: {version}
  count: 1
  elasticsearchRef:
    name: "elasticsearch-sample"
  services:
    - name: api   <1>
      tls:
        certificate:
          secretName: my-cert
----
<1> The service name `api` is reserved for {ls} monitoring endpoint.

[id="{p}-logstash-http-disable-tls"]
=== Disable TLS

You can disable TLS by disabling the generation of the self-signed certificate in the API service definition

[source,yaml,subs="attributes"]
----
apiVersion: logstash.k8s.elastic.co/v1alpha1
kind: Logstash
metadata:
  name: logstash-sample
spec:
  version: {version}
  count: 1
  elasticsearchRef:
    name: "elasticsearch-sample"
  services:
    - name: api
      tls:
        selfSignedCertificate:
          disabled: true
----

[id="{p}-logstash-plugins"]
== {ls} plugins

The power of {ls} is in the plugins--{logstash-ref}/input-plugins.html[inputs], {logstash-ref}/output-plugins.html[outputs], {logstash-ref}/filter-plugins.html[filters,] and {logstash-ref}/codec-plugins.html[codecs].

In {ls} on ECK, you can use the same plugins that you use for other {ls} instances--including Elastic-supported, community-supported, and custom plugins.
However, you may have other factors to consider, such as how you configure your {k8s} resources, how you specify additional resources, and how you scale your {ls} installation.  

In this section, we'll cover: 

* <<{p}-plugin-resources,Providing additional resources for plugins (read-only and writable storage)>>
* <<{p}-logstash-working-with-plugins-scaling>>
* <<{p}-logstash-working-with-plugin-considerations>>
* <<{p}-logstash-working-with-custom-plugins>>

[id="{p}-plugin-resources"]
=== Providing additional resources for plugins

The plugins in your pipeline can impact how you can configure your {k8s} resources, including the need to specify additional resources in your manifest.
The most common resources you need to allow for are:

* Read-only assets, such as private keys, translate dictionaries, or JDBC drivers
* <<{p}-logstash-working-with-plugins-writable>> to save application state

[id="{p}-logstash-working-with-plugins-ro"]
==== Read-only assets

Many plugins require or allow read-only assets in order to work correctly.
These may be ConfigMaps or Secrets files that have a 1 MiB limit, or larger assets such as JDBC drivers, that need to be stored in a PersistentVolume.

[id="{p}-logstash-working-with-plugins-small-ro"]
===== ConfigMaps and Secrets (1 MiB max)

Each instance of a `ConfigMap` or `Secret` has a https://kubernetes.io/docs/concepts/configuration/configmap/#:~:text=The%20data%20stored%20in%20a,separate%20database%20or%20file%20service[maximum size] of 1 MiB (mebibyte). 
For larger read-only assets, check out <<{p}-logstash-working-with-plugins-large-ro>>.

In the plugin documentation, look for configurations that call for a `path` or an `array` of `paths`. 

**Sensitive assets, such as private keys**

Some plugins need access to private keys or certificates in order to access an external resource.
Make the keys or certificates available to the {ls} resource in your manifest.

TIP: These settings are typically identified by an `ssl_` prefix, such as `ssl_key`, `ssl_keystore_path`, `ssl_certificate`, for example.

To use these in your manifest, create a Secret representing the asset, a Volume in your `podTemplate.spec` containing that Secret, and then mount that Volume with a VolumeMount in the `podTemplateSpec.container` section of your {ls} resource.

First, create your secrets.

[source,bash]
----
kubectl create secret generic logstash-crt --from-file=logstash.crt
kubectl create secret generic logstash-key --from-file=logstash.key
----

Then, create your Logstash resource.

[source,yaml]
----
spec:
  podTemplate:
    spec:
      volumes:
        - name: logstash-ssl-crt
          secret:
            secretName: logstash-crt
        - name: logstash-ssl-key
          secret:
            secretName: logstash-key
      containers:
        - name: logstash
          volumeMounts:
            - name: logstash-ssl-key
              mountPath: "/usr/share/logstash/data/logstash.key"
              readOnly: true
            - name: logstash-ssl-crt
              mountPath: "/usr/share/logstash/data/logstash.crt"
              readOnly: true
  pipelines:
    - pipeline.id: main
      config.string: |
        input {
          http {
            port => 8443
            ssl_certificate => "/usr/share/logstash/data/logstash.crt"
            ssl_key => "/usr/share/logstash/data/logstash.key"
          }
        }
----

**Static read-only files**

Some plugins require or allow access to small static read-only files. 
You can use these for a variety of reasons. 
Examples include adding custom `grok` patterns for {logstash-ref}/plugins-filters-grok.html[`logstash-filter-grok`] to use for lookup, source code for [`logstash-filter-ruby`], a dictionary for {logstash-ref}/plugins-filters-translate.html[`logstash-filter-translate`] or the location of a SQL statement for {logstash-ref}/plugins-inputs-jdbc.html[`logstash-input-jdbc`]. 
Make these files available to the {ls} resource in your manifest.

TIP: In the plugin documentation, these plugin settings are typically identified by `path` or an `array` of `paths`.

To use these in your manifest, create a ConfigMap or Secret representing the asset, a Volume in your `podTemplate.spec` containing the ConfigMap or Secret, and mount that Volume with a VolumeMount in your `podTemplateSpec.container` section of your {ls} resource.

This example illustrates configuring a ConfigMap from a ruby source file, and including it in a {logstash-ref}/plugins-filters-ruby.html[`logstash-filter-ruby`] plugin.

First, create the ConfigMap.

[source,bash]
----
kubectl create configmap ruby --from-file=drop_some.rb
----

Then, create your Logstash resource.

[source,yaml]
----
spec:
  podTemplate:
    spec:
      volumes:
        - name: ruby_drop
          configMap:
            name: ruby
      containers:
        - name: logstash
          volumeMounts:
            - name: ruby_drop
              mountPath: "/usr/share/logstash/data/drop_percentage.rb"
              readOnly: true
  pipelines:
    - pipeline.id: main
      config.string: |
        input {
          beats {
            port => 5044
          }
        }
        filter {
          ruby {
            path => "/usr/share/logstash/data/drop_percentage.rb"
            script_params => { "percentage" => 0.9 }
          }
        }
----

[id="{p}-logstash-working-with-plugins-large-ro"]
==== Larger read-only assets (1 MiB+)

Some plugins require or allow access to static read-only files that exceed the 1 MiB (mebibyte) limit imposed by ConfigMap and Secret.
For example, you may need JAR files to load drivers when using a JDBC or JMS plugin, or a large {logstash-ref}/plugins-filters-translate.html[`logstash-filter-translate`] dictionary.

You can add files using:

* **<<{p}-logstash-ic,PersistentVolume populated by an initContainer>>.** Add a volumeClaimTemplate and a volumeMount to your {ls} resource and upload data to that volume, either using an `initContainer`, or direct upload if your Kubernetes provider supports it. 
  You can use the default `logstash-data` volumeClaimTemplate , or a custom one depending on your storage needs.
* **<<{p}-logstash-custom-images,Custom Docker image>>.** Use a custom docker image that includes the static content that your Logstash pods will need.

Check out <<{p}-bundles-plugins>> for more details on which option might be most suitable for you.

[id="{p}-logstash-ic"]
===== Add files using PersistentVolume populated by an initContainer

This example creates a volumeClaimTemplate called `workdir`, with volumeMounts referring to this mounted to the main container and an initContainer. The initContainer initiates a  download of a PostgreSQL JDBC driver JAR file, and stored it the volumeMount, which is then used in the JDBC input in the pipeline configuration.

[source,yaml]
----
spec:
  podTemplate:
    spec:
      initContainers:
      - name: download-postgres
        command: ["/bin/sh"]
        args: ["-c", "curl -o /data/postgresql.jar -L https://jdbc.postgresql.org/download/postgresql-42.6.0.jar"]
        volumeMounts:
          - name: workdir
            mountPath: /data
      containers:
        - name: logstash
          volumeMounts:
            - name: workdir
              mountPath: /usr/share/logstash/jars <1>
  volumeClaimTemplates:
    - metadata:
        name: workdir
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 50Mi
  pipelines:
    - pipeline.id: main
      config.string: |
        input {
          jdbc {
             jdbc_driver_library => "/usr/share/logstash/jars/postgresql.jar"
             jdbc_driver_class => "org.postgresql.Driver"
             <2>
          }
        }
----
<1> Should match the `mountPath` of the `container`
<2> Remainder of plugin configuration goes here

[id="{p}-logstash-custom-images"]
===== Add files using a custom Docker image

This example downloads the same `postgres` JDBC driver, and adds it to the {ls} classpath in the Docker image.

First, create a Dockerfile based on the {ls} Docker image.
Download the JDBC driver, and save it alongside the other JAR files in the {ls} classpath:


["source","shell",subs="attributes"]
----
FROM docker.elastic.co/logstash/logstash:{version}
RUN curl -o /usr/share/logstash/logstash-core/lib/jars/postgresql.jar -L https://jdbc.postgresql.org/download/postgresql-42.6.0.jar <1>
----
<1> Placing the JAR file in the `/usr/share/logstash/logstash-core/lib/jars` folder adds it to the {ls} classpath.

After you build and deploy the custom image, include it in the {ls} manifest.
Check out <<{p}-custom-images>> for more details.

[source,yaml]
----
  count: 1
  version: {version} <1>
  image: <CUSTOM_IMAGE>
  pipelines:
    - pipeline.id: main
      config.string: |
        input {
          jdbc {
              <2>
             jdbc_driver_class => "org.postgresql.Driver"
              <3>
          }
        }

----
<1> The correct version is required as ECK reasons about available APIs and capabilities based on the version field.
<2> Note that when you place the JAR file on the {ls} classpath, you do not need to specify the `jdbc_driver_library` location in the plugin configuration.
<3> Remainder of plugin configuration goes here

[id="{p}-logstash-working-with-plugins-writable"]
==== Writable storage

Some {ls} plugins need access to writable storage. 
This could be for checkpointing to keep track of events already processed, a place to temporarily write events before sending a batch of events, or just to actually write events to disk in the case of {logstash-ref}/plugins-outputs-file.html[`logstash-output-file`].

{ls} on ECK by default supplies a small 1.5 GiB (gibibyte) default persistent volume to each pod.
This volume is called `logstash-data` and is located at `/usr/logstash/data`, and is typically the default location for most plugin use cases. 
This volume is stable across restarts of {ls} pods and is suitable for many use cases.

NOTE: When plugins use writable storage, each plugin must store its data a dedicated folder or file to avoid overwriting data.

[id="{p}-logstash-working-with-plugins-writable-checkpointing"]
===== Checkpointing

Some {ls} plugins need to write "checkpoints" to local storage in order to keep track of events that have already been processed. 
Plugins that retrieve data from external sources need to do this if the external source does not provide any mechanism to track state internally.

Not all external data sources have mechanisms to track state internally, and {ls} checkpoints can help persist data. 

In the plugin documentation, look for configurations that call for a `path` with a settings like `sincedb`, `sincedb_path`, `sequence_path`, or `last_run_metadata_path`. Check out specific plugin documentation in the {logstash-ref}[Logstash Reference] for details.

[source, yaml,subs="attributes,+macros,callouts"]
----
spec:
  pipelines:
    - pipeline.id: main
      config.string: |
        input {
          jdbc {
             jdbc_driver_library => "/usr/share/logstash/jars/postgresql.jar"
             jdbc_driver_class => "org.postgresql.Driver"
             last_metadata_path => "/usr/share/logstash/data/main/logstash_jdbc_last_run <1>
          }
        }
----
<1> If you are using more than one plugin of the same type, specify a unique location for each plugin to use.

If the default `logstash-data` volume is insufficient for your needs, see the volume section for details on how to add additional volumes.


[id="{p}-logstash-working-with-plugins-writable-temp"]
===== Writable staging or temporary data

Some {ls} plugins write data to a staging directory or file before processing for input, or outputting to their final destination. 
Often these staging folders can be persisted across restarts to avoid duplicating processing of data.

In the plugin documentation, look for  names such as `tmp_directory`, `temporary_directory`, `staging_directory`.

To persist data across pod restarts, set this value to point to the default `logstash-data` volume or your own PersistentVolumeClaim. 

[source, yaml,subs="attributes,+macros,callouts"]
----
spec:
  pipelines:
    - pipeline.id: main
      config.string: |
        output {
          s3 {
             id => "main_s3_output"
             temporary_directory => "/usr/share/logstash/data/main/main_s3_output<1>
          }
        }
----
<1> If you are using more than one plugin of the same type, specify a unique location for each plugin to use.

[id="{p}-logstash-working-with-plugins-scaling"]
=== Scaling {ls} on ECK

IMPORTANT: The use of autoscalers, such as the HorizontalPodAutoscaler or the VerticalPodAutoscaler, with {ls} on ECK is not yet supported.

{ls} scalability is highly dependent on the plugins in your pipelines. 
Some plugins can restrict how you can scale out your Logstash deployment, based on the way that the plugins gather or enrich data.

Plugin categories that require special considerations are:

* <<{p}-logstash-agg-filters>>
* <<{p}-logstash-inputs-data-pushed>>
* <<{p}-logstash-inputs-local-checkpoints>>
* <<{p}-logstash-inputs-external-state>>

If the pipeline _does not_ contain any plugins from these categories, you can increase the number of {ls} instances by setting the `count` property in the {ls} resource:

[source,yaml,subs="attributes,+macros,callouts"]
----
apiVersion: logstash.k8s.elastic.co/v1alpha1
kind: Logstash
metadata:
  name: quickstart
spec:
  version: {version}
  count: 3
----

.Horizontal scaling for {ls} plugins
****
* Not all {ls} deployments can be scaled horizontally by increasing the number of {ls} Pods defined in the {ls} resource. 
Depending on the types of plugins in a {ls} installation, increasing the number of pods may cause data duplication, data loss, incorrect data, or may waste resources with pods unable to be utilized correctly.

* The ability of a {ls} installation to scale horizontally is bound by its most restrictive plugin(s). Even if all pipelines are using {logstash-ref}/plugins-inputs-elastic_agent.html[`logstash-input-elastic_agent`] or {logstash-ref}/plugins-inputs-beats.html[`logstash-input-beats`] which should enable full horizontal scaling, introducing a more restrictive input or filter plugin forces the restrictions for pod scaling associated with that plugin.
****

[id="{p}-logstash-agg-filters"]
==== Filter plugins: aggregating filters

{ls} installations that use aggregating filters should be treated with particular care: 

* They *must* specify `pipeline.workers=1` for any pipelines that use them.
* The number of pods cannot be scaled above 1.

Examples of aggregating filters include {logstash-ref}/plugins-filters-aggregate.html[`logstash-filter-aggregate`], {logstash-ref}/plugins-filters-csv.html[`logstash-filter-csv`] when `autodetect_column_names` set to `true`, and any {logstash-ref}/plugins-filters-ruby.html[`logstash-filter-ruby`] implementations that perform aggregations.

[id="{p}-logstash-inputs-data-pushed"]
==== Input plugins: events pushed to {ls}

{ls} installations with inputs that enable {ls} to receive data should be able to scale freely and have load spread across them horizontally.
These plugins include {logstash-ref}/plugins-inputs-beats.html[`logstash-input-beats`], {logstash-ref}/plugins-inputs-elastic_agent.html[`logstash-input-elastic_agent`],  {logstash-ref}/plugins-inputs-tcp.html[`logstash-input-tcp`], and {logstash-ref}/plugins-inputs-http.html[`logstash-input-http`].

[id="{p}-logstash-inputs-local-checkpoints"]
==== Input plugins: {ls} maintains state 

{ls} installations that use input plugins that retrieve data from an external source, and **maintain local checkpoint state**, or would require some level of co-ordination between nodes to split up work can specify `pipeline.workers` freely, but should keep the pod count at 1 for each {ls} installation.

Note that plugins that retrieve data from external sources, and require some level of coordination between nodes to split up work, are not good candidates for scaling horizontally, and would likely produce some data duplication. 

Input plugins that include configuration settings such as  `sincedb`, `checkpoint` or `sql_last_run_metadata` may fall into this category.

Examples of these plugins include {logstash-ref}/plugins-inputs-jdbc.html[`logstash-input-jdbc`] (which has no automatic way to split queries across {ls} instances), {logstash-ref}/plugins-inputs-s3.html[`logstash-input-s3`] (which has no way to split which buckets to read across {ls} instances), or {logstash-ref}/plugins-inputs-file.html[`logstash-input-file`].

[id="{p}-logstash-inputs-external-state"]
==== Input plugins: external source stores state

{ls} installations that use input plugins that retrieve data from an external source, and **rely on the external source to store state** can scale based on the parameters of the external source.

For example, a {ls} installation that uses a {logstash-ref}/plugins-inputs-kafka.html[`logstash-input-kafka`] plugin to retrieve data can scale the number of pods up to the number of partitions used, as a partition can have at most one consumer belonging to the same consumer group. 
Any pods created beyond that threshold cannot be scheduled to receive data.

Examples of these plugins include {logstash-ref}/plugins-inputs-kafka.html[`logstash-input-kafka`], {logstash-ref}/plugins-inputs-azure_event_hubs.html[`logstash-input-azure_event_hubs`], and {logstash-ref}/plugins-inputs-kinesis.html[`logstash-input-kinesis`].

[id="{p}-logstash-working-with-plugin-considerations"]
=== Plugin-specific considerations

Some plugins have additional requirements and guidelines for optimal performance in a {ls} ECK environment.

* <<{p}-logstash-plugin-considerations-ls-integration>>
* <<{p}-logstash-plugin-considerations-es-output>>
* <<{p}-logstash-plugin-considerations-integration-filter>>
* <<{p}-logstash-plugin-considerations-agent-beats>>

TIP: Use these guidelines _in addition_ to the general guidelines provided in <<{p}-logstash-working-with-plugins-scaling>>.

[id="{p}-logstash-plugin-considerations-ls-integration"]
==== {ls} integration plugin 

When your pipeline uses the {logstash-ref}/plugins-integrations-logstash.html[`Logstash integration`] plugin, add `keepalive=>false` to the {logstash-ref}/plugins-outputs-logstash.html[logstash-output] definition to ensure that load balancing works correctly rather than keeping affinity to the same pod.

[id="{p}-logstash-plugin-considerations-es-output"]
==== Elasticsearch output plugin

The {logstash-ref}/plugins-outputs-elasticsearch.html[`elasticsearch output`] plugin requires certain roles to be configured in order to enable {ls} to communicate with {es}.

You can customize roles in {es}. Check out <<{p}-users-and-roles,creating custom roles>>

[source, logstash]
----
kind: Secret
apiVersion: v1
metadata:
  name: my-roles-secret
stringData:
  roles.yml: |-
    eck_logstash_user_role:
      "cluster": ["monitor", "manage_ilm", "read_ilm", "manage_logstash_pipelines", "manage_index_templates", "cluster:admin/ingest/pipeline/get"],
      "indices": [
        {
          "names": [ "logstash", "logstash-*", "ecs-logstash", "ecs-logstash-*", "logs-*", "metrics-*", "synthetics-*", "traces-*" ],
          "privileges": ["manage", "write", "create_index", "read", "view_index_metadata"]
        }
      ]
----


[id="{p}-logstash-plugin-considerations-integration-filter"]
==== Elastic_integration filter plugin

The {logstash-ref}/plugins-filters-elastic_integration.html[`elastic_integration filter`] plugin allows the use of <<k8s-logstash-esref,`ElasticsearchRef`>> and environment variables.

[source, logstash]
----
  elastic_integration {
            pipeline_name => "logstash-pipeline"
            hosts => [ "${ECK_ES_HOSTS}" ]
            username => "${ECK_ES_USER}"
            password => "${ECK_ES_PASSWORD}"
            ssl_certificate_authorities => "${ECK_ES_SSL_CERTIFICATE_AUTHORITY}"
          }

----

The Elastic_integration filter requires certain roles to be configured on the {es} cluster to enable {ls} to read ingest pipelines.

[source, yaml,subs="attributes,+macros,callouts"]
----
# Sample role definition
kind: Secret
apiVersion: v1
metadata:
  name: my-roles-secret
stringData:
  roles.yml: |-
    eck_logstash_user_role:
      cluster: [ "monitor", "manage_index_templates", "read_pipeline"]
----

[id="{p}-logstash-plugin-considerations-agent-beats"]
==== Elastic Agent input and Beats input plugins

When you use the {logstash-ref}/plugins-inputs-elastic_agent.html[Elastic Agent input] or the {logstash-ref}/plugins-inputs-beats.html[Beats input],
set the {filebeat-ref}/logstash-output.html#_ttl[`ttl`] value on the Agent or Beat to ensure that load is distributed appropriately.

[id="{p}-logstash-working-with-custom-plugins"]
=== Adding custom plugins

If you need plugins in addition to those included in the standard {ls} distribution, you can add them. 
Create a custom Docker image that includes the installed plugins, using the `bin/logstash-plugin install` utility to add more plugins to the image so that they can be used by {ls} pods.

This sample Dockerfile installs the {logstash-ref}/plugins-filters-tld.html[`logstash-filter-tld`] plugin to the official {ls} Docker image:

["source","shell",subs="attributes"]
----
FROM docker.elastic.co/logstash/logstash:{version}
RUN bin/logstash-plugin install logstash-filter-tld
----

Then after building and deploying the custom image (refer to <<{p}-custom-images>> for more details), include it in the {ls} manifest:

["source","shell",subs="attributes"]
----
spec:
  count: 1
  version: {version} <1>
  image: <CUSTOM_IMAGE>
----
<1> The correct version is required as ECK reasons about available APIs and capabilities based on the version field.



[id="{p}-logstash-configuration-examples"]
== Configuration examples

This section contains manifests that illustrate common use cases, and can be your starting point in exploring Logstash deployed with ECK. These manifests are self-contained and work out-of-the-box on any non-secured Kubernetes cluster. They all contain a three-node Elasticsearch cluster and a single Kibana instance.

CAUTION: The examples in this section are for illustration purposes only. They should not be considered production-ready. 
Some of these examples use the `node.store.allow_mmap: false` setting on {es} which has performance implications and should be tuned for production workloads, as described in <<{p}-virtual-memory>>.


[id="{p}-logstash-configuration-single-pipeline-crd"]
=== Single pipeline defined in CRD

[source,sh,subs="attributes"]
----
kubectl apply -f {logstash_recipes}/logstash-eck.yaml
----

Deploys Logstash with a single pipeline defined in the CRD

[id="{p}-logstash-configuration-single-pipeline-secret"]
=== Single Pipeline defined in Secret

[source,sh,subs="attributes"]
----
kubectl apply -f {logstash_recipes}/logstash-pipeline-as-secret.yaml
----

Deploys Logstash with a single pipeline defined in a secret, referenced by a `pipelineRef`

[id="{p}-logstash-configuration-pipeline-volume"]
=== Pipeline configuration in mounted volume

[source,sh,subs="attributes"]
----
kubectl apply -f {logstash_recipes}/logstash-pipeline-as-volume.yaml
----

Deploys Logstash with a single pipeline defined in a secret, mounted as a volume, and referenced by
`path.config`

[id="{p}-logstash-configuration-custom-index"]
=== Writing to a custom Elasticsearch index

[source,sh,subs="attributes"]
----
kubectl apply -f {logstash_recipes}/logstash-es-role.yaml
----

Deploys Logstash and Elasticsearch, and creates an updated version of the `eck_logstash_user_role` to write to a user specified index.

[id="{p}-logstash-configuration-pq-dlq"]
=== Creating persistent volumes for PQ and DLQ

[source,sh,subs="attributes"]
----
kubectl apply -f {logstash_recipes}/logstash-volumes.yaml
----

Deploys Logstash, Beats and Elasticsearch. Logstash is configured with two pipelines:

* a main pipeline for reading from the {beats} instance, which will send to the DLQ if it is unable to write to Elasticsearch
* a second pipeline, that will read from the DLQ.
In addition, persistent queues are set up.
This example shows how to configure persistent volumes outside of the default `logstash-data` persistent volume.


[id="{p}-logstash-configuration-stack-monitoring"]
=== Elasticsearch and Kibana Stack Monitoring

[source,sh,subs="attributes"]
----
kubectl apply -f {logstash_recipes}/logstash-monitored.yaml
----

Deploys an Elasticsearch and Kibana monitoring cluster, and a Logstash that will send its monitoring information to this cluster. You can view the stack monitoring information in the monitoring cluster's Kibana

[id="{p}-logstash-configuration-multiple-pipelines"]
=== Multiple pipelines/multiple Elasticsearch clusters

[source,sh,subs="attributes"]
----
kubectl apply -f {logstash_recipes}/logstash-multi.yaml
----

Deploys Elasticsearch in prod and qa configurations, running in separate namespaces. Logstash is configured with a multiple pipeline->pipeline configuration, with a source pipeline routing to `prod` and `qa` pipelines.


[id="{p}-logstash-update-strategy"]
== Update Strategy

The operator takes a Pod down to restart and applies a new configuration value. All Pods are restarted in reverse ordinal order.

=== Default behavior

When `updateStrategy` is not present in the specification, it defaults to the following:

[source,yaml,subs="attributes,+macros,callouts"]
----
spec:
  updateStrategy:
    type: "RollingUpdate" <1>
    rollingUpdate:
      partition: 0        <2>
      maxUnavailable: 1   <3>
----

<1> The `RollingUpdate` strategy will update Pods one by one in reverse ordinal order.
<2> This means that all the Pods from ordinal Replicas-1 to `partition` are updated . You can split the update into partitions to perform link:https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/#rolling-out-a-canary[canary rollout].
<3> This ensures that the cluster has no more than one unavailable Pod at any given point in time.

=== OnDelete

[source,yaml]
----
spec:
  updateStrategy:
    type: "OnDelete"
----

`OnDelete` strategy does not automatically update Pods when a modification is made. You need to restart Pods yourself.


[id="{p}-logstash-advanced-configuration"]
== Advanced configuration

[id="{p}-logstash-jvm-options"]
=== Setting JVM options

You can change JVM settings by using the `LS_JAVA_OPTS` environment variable to override default settings in `jvm.options`. This approach ensures that expected settings from `jvm.options` are set, and only options that explicitly need to be overridden are.

To do, this, set the  `LS_JAVA_OPTS` environment variable in the container definition of your Logstash resource:

[source,yaml,subs="attributes,+macros,callouts"]
----
apiVersion: logstash.k8s.elastic.co/v1alpha1
kind: Logstash
metadata:
  name: quickstart
spec:
  podTemplate:
    spec:
      containers:
        - name: logstash
          env:
            - name: LS_JAVA_OPTS   <1>
              value: "-Xmx2g -Xms2g"
----
<1> This will change the maximum and minimum heap size of the JVM on each pod to 2GB

[id="{p}-logstash-keystore"]
=== Setting keystore

You can specify sensitive settings with Kubernetes secrets. ECK automatically injects these settings into the keystore before it starts Logstash.
The ECK operator continues to watch the secrets for changes and will restart Logstash Pods when it detects a change.

The Logstash Keystore can be password protected by setting an environment variable called `LOGSTASH_KEYSTORE_PASS`. Check out https://www.elastic.co/guide/en/logstash/current/keystore.html#keystore-password[Logstash Keystore] documentation for details.

[source,yaml,subs="attributes,+macros,callouts"]
----
apiVersion: v1
kind: Secret
metadata:
  name: logstash-keystore-pass
stringData:
  LOGSTASH_KEYSTORE_PASS: changed   <1>
---
apiVersion: v1
kind: Secret
metadata:
  name: logstash-secure-settings
stringData:
  HELLO: Hallo
---
apiVersion: logstash.k8s.elastic.co/v1alpha1
kind: Logstash
metadata:
  name: logstash-sample
spec:
  version: {version}
  count: 1
  pipelines:
    - pipeline.id: main
      config.string: |-
        input { exec { command => 'uptime' interval => 10 } }
        filter {
          if ("${HELLO:}" != "") {   <2>
            mutate { add_tag => ["awesome"] }
          }
        }
  secureSettings:
    - secretName: logstash-secure-settings
  podTemplate:
    spec:
      containers:
        - name: logstash
          env:
            - name: LOGSTASH_KEYSTORE_PASS
              valueFrom:
                secretKeyRef:
                  name: logstash-keystore-pass
                  key: LOGSTASH_KEYSTORE_PASS
----
<1> Value of password to protect the Logstash keystore
<2> The syntax for referencing keys is identical to the syntax for environment variables

