:page_id: common-problems
ifdef::env-github[]
****
link:https://www.elastic.co/guide/en/cloud-on-k8s/master/k8s-{page_id}.html[View this document on the Elastic website]
****
endif::[]
[id="{p}-{page_id}"]
= Common problems

[id="{p}-{page_id}-operator-oom"]
== Operator crashes on startup with `OOMKilled`

On very large Kubernetes clusters with many hundreds of resources (pods, secrets, config maps, and so on), the operator may fail to start with its pod getting killed with a `OOMKilled` message. This is an issue with the `controller-runtime` framework on top of which the operator is built. Even though the operator is only interested in the resources created by itself, the framework code needs to gather information about all relevant resources in the Kubernetes cluster in order to provide the filtered view of cluster state required by the operator. On very large clusters, this information gathering can use up a lot of memory and exceed the default resource limit defined for the operator pod.

The default link:https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#meaning-of-memory[memory limit] for the operator pod is set to 512 MiB. You can increase (or decrease) this limit to a value suited to your cluster as follows:

[source,sh]
----
kubectl patch sts elastic-operator -n elastic-system -p '{"spec":{"template":{"spec":{"containers":[{"name":"manager", "resources":{"limits":{"memory":"768Mi"}}}]}}}}'
----


[id="{p}-{page_id}-webhook-timeout"]
== Timeout when submitting a resource manifest

When submitting a ECK resource manifest, you may encounter an error message similar to the following:

....
Error from server (Timeout): error when creating "elasticsearch.yaml": Timeout: request did not complete within requested timeout 30s
....


This error is usually an indication of a problem communicating with the validating webhook. If you are running ECK on a private Google Kubernetes Engine (GKE) cluster, you may need to link:https://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters#add_firewall_rules[add a firewall rule] allowing port 9443 from the API server. Another possible cause for failure is if a strict network policy is in effect. Refer to the <<{p}-webhook-troubleshooting-timeouts,webhook troubleshooting>> documentation for  more details and workarounds.

[id="{p}-{page_id}-owner-refs"]
== Copying secrets with Owner References

Copying the Elasticsearch Secrets generated by ECK (for instance, the certificate authority or the elastic user) into another namespace wholesale can trigger a link:https://github.com/kubernetes/kubernetes/issues/65200[Kubernetes bug] which can delete all of the Elasticsearch-related resources (for instance, the data volumes). To avoid this, when copying Secrets between namespaces, remove the `metadata.ownerReferences` section. For example, a source secret might be:

[source,yaml,subs="attributes"]
----
kubectl get secret quickstart-es-elastic-user -o yaml
apiVersion: v1
data:
  elastic: NGw2Q2REMjgwajZrMVRRS0hxUDVUUTU0
kind: Secret
metadata:
  creationTimestamp: "2020-06-09T19:11:41Z"
  labels:
    common.k8s.elastic.co/type: elasticsearch
    eck.k8s.elastic.co/credentials: "true"
    elasticsearch.k8s.elastic.co/cluster-name: quickstart
  name: quickstart-es-elastic-user
  namespace: default
  ownerReferences:
  - apiVersion: elasticsearch.k8s.elastic.co/{eck_crd_version}
    blockOwnerDeletion: true
    controller: true
    kind: Elasticsearch
    name: quickstart
    uid: c7a9b436-aa07-4341-a2cc-b33b3dfcbe29
  resourceVersion: "13048277"
  selfLink: /api/v1/namespaces/default/secrets/quickstart-es-elastic-user
  uid: 04cdf334-77d3-4de6-a2e8-7a2b23366a27
type: Opaque
----

To copy it to a different namespace, strip the `metadata.ownerReferences` field as well as the object-specific data:

[source,yaml]
----
apiVersion: v1
data:
  elastic: NGw2Q2REMjgwajZrMVRRS0hxUDVUUTU0
kind: Secret
metadata:
  labels:
    common.k8s.elastic.co/type: elasticsearch
    eck.k8s.elastic.co/credentials: "true"
    elasticsearch.k8s.elastic.co/cluster-name: quickstart
  name: quickstart-es-elastic-user
  namespace: default
type: Opaque
----

Failure to do so can cause data loss.

[id="{p}-{page_id}-scale-down"]
== Scale down of Elasticsearch master-eligible Pods seems stuck

If a master-eligible Elasticsarch Pod was never successfully scheduled and the Elasticsearch cluster is running version 7.8 or earlier, ECK may fail to scale down the Pod. To find out whether you are affected, check if the Pod in question is pending:
[source,sh]
----
> kubectl get pods
pod/<cluster-name>-es-<nodeset>-1                    0/1     Pending   0          26m    <none>        <none>
----

Check the <<{p}-get-eck-logs,operator logs>> for an error similar to:
[source,sh]
----
"unable to add to voting_config_exclusions: 400 Bad Request: add voting config exclusions request for [<cluster-name>-es-<nodeset>-1] matched no master-eligible nodes",
----

To work around this issue, scale down the underlying StatefulSet manually. First, identify the affected StatefulSet and the number of Pods that are ready (symbolized by `m` in this example):

[source,sh]
----
> kubectl get sts -l elasticsearch.k8s.elastic.co/cluster-name=<cluster-name>
NAME                       READY   AGE
<cluster-name>-es-<nodeset>   m/n     44h
----
Then, scale down the StatefulSet to the right size `m`, removing the pending Pod:
[source,sh]
----
> kubectl scale --replicas=m  sts/<cluster-name>-es-<nodeset>
----

CAUTION: Do not use this method to scale down Pods that have already joined the Elasticsearch cluster, as additional data loss protection that ECK applies is sidestepped.

[id="{p}-{page_id}-pod-updates"]
== Pods are not replaced after a configuration update

The update of an existing Elasticsearch cluster configuration can fail because the operator is unable to apply the changes required while replacing the pods of a given Elasticsearch cluster.

A key indicator is when the Phase of the Elasticsearch resource is in `ApplyingChanges` state for too long:

[source,sh]
----
kubectl get es

NAME                  HEALTH   NODES    VERSION   PHASE            AGE          
elasticsearch-sample  yellow   2        7.9.2     ApplyingChanges  36m
----

Possible causes include:

* The Elasticsearch cluster is not healthy
+
[source,sh]
----
kubectl get elasticsearch

NAME                                                              HEALTH   NODES   VERSION   PHASE   AGE
elasticsearch.elasticsearch.k8s.elastic.co/elasticsearch-sample   yellow   1       7.9.2     Ready   3m50s
----
+
In this case, you have to link:{ref}/cluster-allocation-explain.html[check] and fix your shard allocations.  The link:{ref}/cluster-health.html[cluster health], link:{ref}/cat-shards.html[cat shards], and <<{p}-elasticsearch-monitor-cluster-health,get Elasticsearch>> APIs can assist in tracking the shard recover process.

* Scheduling issues
+
The scheduling fails with the following message:
+
[source,sh]
----
kubectl get events --sort-by='{.lastTimestamp}' | tail

LAST SEEN   TYPE      REASON             OBJECT                        MESSAGE
10s         Warning   FailedScheduling   pod/quickstart-es-default-2   0/3 nodes are available: 3 Insufficient memory.
----
+
As an alternative, to get more specific information about a given pod, you can use the following command: 
+
[source,sh]
----
kubectl get pod elasticsearch-sample-es-default-2  -o go-template="{{.status}}"
map[conditions:[map[lastProbeTime:<nil> lastTransitionTime:2020-12-07T09:31:06Z message:0/3 nodes are available: 3 Insufficient cpu. reason:Unschedulable status:False type:PodScheduled]] phase:Pending qosClass:Guaranteed]
----


* The operator is not able to restart some nodes
+
[source,sh]
----
kubectl -n elastic-system logs statefulset.apps/elastic-operator | tail

{"log.level":"info","@timestamp":"2020-11-19T17:34:48.769Z","log.logger":"driver","message":"Cannot restart some nodes for upgrade at this time","service.version":"1.3.0+6db1914b","service.type":"eck","ecs.version":"1.4.0","namespace":"default","es_name":"quickstart","failed_predicates":{"do_not_restart_healthy_node_if_MaxUnavailable_reached":["quickstart-es-default-1","quickstart-es-default-0"]}}
----
+
A pod is stuck in a `Pending` status:
+
[source,sh]
----
kubectl get pods

NAME                      READY   STATUS    RESTARTS   AGE
quickstart-es-default-0   1/1     Running   0          146m
quickstart-es-default-1   1/1     Running   0          146m
quickstart-es-default-2   0/1     Pending   0          134m
----
+
In this case, you have to add more K8s nodes, or free up resources.

For more information, see <<{p}-troubleshooting-methods>>.
