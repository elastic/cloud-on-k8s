---
eck-elasticsearch:
  nodeSets:
  - name: masters
    count: 1
    config:
      node.roles: ["master"]
      # Comment out when setting the vm.max_map_count via initContainer, as these are mutually exclusive.
      # For production workloads, it is strongly recommended to increase the kernel setting vm.max_map_count to 262144
      # and leave node.store.allow_mmap unset.
      # ref: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-virtual-memory.html
      #
      node.store.allow_mmap: false
    podTemplate:
      spec:
        containers:
        - name: elasticsearch
          resources:
            limits:
              memory: 8Gi
              cpu: 2
        # Affinity/Anti-affinity settings for controlling the 'spreading' of Elasticsearch
        # pods across existing hosts.
        # ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
        # ref: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-advanced-node-scheduling.html#k8s-affinity-options
        #
        # affinity:
        #   nodeAffinity:
        #     requiredDuringSchedulingIgnoredDuringExecution:
        #       nodeSelectorTerms:
        #       - matchExpressions:
        #         - key: beta.kubernetes.io/instance-type
        #           operator: In
        #           # This should be adjusted to the instance type according to your setup
        #           #
        #           values:
        #           - highio
    # Volume Claim settings.
    # ref: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-volume-claim-templates.html
    #
    volumeClaimTemplates:
    - metadata:
        name: elasticsearch-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 1Ti
        # Adjust to your storage class name
        #
        # storageClassName: local-storage
  - name: hot
    count: 1
    config:
      node.roles: ["data_hot", "data_content", "ingest"]
      # Comment out when setting the vm.max_map_count via initContainer, as these are mutually exclusive.
      # For production workloads, it is strongly recommended to increase the kernel setting vm.max_map_count to 262144
      # and leave node.store.allow_mmap unset.
      # ref: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-virtual-memory.html
      #
      node.store.allow_mmap: false
    podTemplate:
      spec:
        containers:
        - name: elasticsearch
          resources:
            limits:
              memory: 16Gi
              cpu: 4
        # Affinity/Anti-affinity settings for controlling the 'spreading' of Elasticsearch
        # pods across existing hosts.
        # ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
        # ref: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-advanced-node-scheduling.html#k8s-affinity-options
        #
        # affinity:
        #   nodeAffinity:
        #     requiredDuringSchedulingIgnoredDuringExecution:
        #       nodeSelectorTerms:
        #       - matchExpressions:
        #         - key: beta.kubernetes.io/instance-type
        #           operator: In
        #           # This should be adjusted to the instance type according to your setup
        #           #
        #           values:
        #           - highio
    # Volume Claim settings.
    # ref: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-volume-claim-templates.html
    #
    volumeClaimTemplates:
    - metadata:
        name: elasticsearch-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 1Ti
        # Adjust to your storage class name
        #
        # storageClassName: local-storage
  - name: warm
    count: 1
    config:
      node.roles: ["data_warm"]
      # Comment out when setting the vm.max_map_count via initContainer, as these are mutually exclusive.
      # For production workloads, it is strongly recommended to increase the kernel setting vm.max_map_count to 262144
      # and leave node.store.allow_mmap unset.
      # ref: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-virtual-memory.html
      #
      node.store.allow_mmap: false
    podTemplate:
      spec:
        containers:
        - name: elasticsearch
          resources:
            limits:
              memory: 16Gi
              cpu: 2
        # Affinity/Anti-affinity settings for controlling the 'spreading' of Elasticsearch
        # pods across existing hosts.
        # ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
        # ref: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-advanced-node-scheduling.html#k8s-affinity-options
        #
        # affinity:
        #   nodeAffinity:
        #     requiredDuringSchedulingIgnoredDuringExecution:
        #       nodeSelectorTerms:
        #       - matchExpressions:
        #         - key: beta.kubernetes.io/instance-type
        #           operator: In
        #           # This should be adjusted to the instance type according to your setup
        #           #
        #           values:
        #           - highstorage
    # Volume Claim settings.
    # ref: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-volume-claim-templates.html
    #
    volumeClaimTemplates:
    - metadata:
        name: elasticsearch-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 10Ti
        # Adjust to your storage class name
        #
        # storageClassName: local-storage
  - name: cold
    count: 1
    config:
      node.roles: ["data_cold"]
      # Comment out when setting the vm.max_map_count via initContainer, as these are mutually exclusive.
      # For production workloads, it is strongly recommended to increase the kernel setting vm.max_map_count to 262144
      # and leave node.store.allow_mmap unset.
      # ref: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-virtual-memory.html
      #
      node.store.allow_mmap: false
    podTemplate:
      spec:
        containers:
        - name: elasticsearch
          resources:
            limits:
              memory: 8Gi
              cpu: 2
        # Affinity/Anti-affinity settings for controlling the 'spreading' of Elasticsearch
        # pods across existing hosts.
        # ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
        # ref: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-advanced-node-scheduling.html#k8s-affinity-options
        #
        # affinity:
        #   nodeAffinity:
        #     requiredDuringSchedulingIgnoredDuringExecution:
        #       nodeSelectorTerms:
        #       - matchExpressions:
        #         - key: beta.kubernetes.io/instance-type
        #           operator: In
        #           # This should be adjusted to the instance type according to your setup
        #           #
        #           values:
        #           - highstorage
    # Volume Claim settings.
    # ref: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-volume-claim-templates.html
    #
    volumeClaimTemplates:
    - metadata:
        name: elasticsearch-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 20Ti
        # Adjust to your storage class name
        #
        # storageClassName: local-storage
