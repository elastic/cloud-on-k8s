---
apiVersion: v1
kind: ConfigMap
metadata:
  name: local-provisioner-config
  namespace: default
data:
  useNodeNameOnly: "true"
  storageClassMap: |
    e2e-default:
       hostDir: /mnt/disks/pvs
       mountDir:  /mnt/disks/pvs
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: setup-disks
data:
  format.sh: |-
    #!/bin/bash
    # Mounts Amazon EC2 NVMe instance storage as ext4 to /mnt/ssd[0-9]
    # Inspired by https://github.com/brunsgaard/eks-nvme-ssd-provisioner/blob/master/eks-nvme-ssd-provisioner.sh

    set -o errexit
    set -o nounset
    set -o pipefail

    # TODO custom docker image
    apt-get update && apt-get -y install nvme-cli

    SSD_NVME_DEVICE_LIST=($(nvme list | grep "Amazon EC2 NVMe Instance Storage" | cut -d " " -f 1 || true))
    SSD_NVME_DEVICE_COUNT=${#SSD_NVME_DEVICE_LIST[@]}
    FILESYSTEM_BLOCK_SIZE=${FILESYSTEM_BLOCK_SIZE:-4096}  # Bytes

    # Checking if provisioning already happend
    if [[ "$(ls -A /mnt/disks)" ]]
    then
      echo 'Volumes already present in "/mnt/disks"'
      echo -e "\n$(ls -Al /mnt/disks | tail -n +2)\n"
      echo "I assume that provisioning already happend, doing nothing!"
      exit 0
    fi

    # Perform provisioning based on nvme device count
    case $SSD_NVME_DEVICE_COUNT in
    "0")
      echo 'No devices found of type "Amazon EC2 NVMe Instance Storage"'
      exit 1
      ;;
    *)
      for (( i=0; i<${SSD_NVME_DEVICE_COUNT}; i++ ));
      do
        DEVICE=${SSD_NVME_DEVICE_LIST[$i]}
        mkfs.ext4 -m 0 -b $FILESYSTEM_BLOCK_SIZE $DEVICE
        DIR=ssd$i
        MNTPOINT=/mnt/disks/$DIR
        UUID=$(blkid -s UUID -o value $DEVICE)
        mkdir -p $MNTPOINT
        mount -o defaults,noatime,discard,nobarrier --uuid $UUID $MNTPOINT
      done
      ;;
    esac

    echo "NVMe SSD provisioning is done"
  entrypoint.sh: |-
    #!/bin/sh
    for i in $(seq 1 10);
    do
    disk_name="/mnt/disks/ssd0/disk-$i"
    mount_point="/mnt/disks/pvs/pv-$i"
    echo "Creating pv-$i"
    mkdir -p "${disk_name}" "${mount_point}"
    if mountpoint -q -- "${mount_point}"; then
      echo "${mount_point} already mounted"
    else
      echo "Binding ${disk_name} to ${mount_point}"
      mount --bind "${disk_name}" "${mount_point}"
    fi
    done
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: local-volume-provisioner
  namespace: default
  labels:
    app: local-volume-provisioner
spec:
  selector:
    matchLabels:
      app: local-volume-provisioner
  template:
    metadata:
      labels:
        app: local-volume-provisioner
    spec:
      serviceAccountName: local-storage-admin
      initContainers:
      - name: disk-format
        image: debian:stretch-slim
        securityContext:
          privileged: true
        command:
        - "/bin/format.sh"
        volumeMounts:
          - mountPath: /bin/format.sh
            name: setup-disks
            subPath: format.sh
          - mountPath:  /mnt/disks
            name: e2e-default
            mountPropagation: Bidirectional
      - name: disk-mounts
        image: busybox
        securityContext:
          privileged: true
        command:
        - "/bin/entrypoint.sh"
        volumeMounts:
          - mountPath: /bin/entrypoint.sh
            name: setup-disks
            subPath: entrypoint.sh
          - mountPath:  /mnt/disks
            name: e2e-default
            mountPropagation: Bidirectional
      containers:
        - image: "quay.io/external_storage/local-volume-provisioner:v2.3.4"
          imagePullPolicy: "Always"
          name: provisioner
          securityContext:
            privileged: true
          env:
          - name: MY_NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          volumeMounts:
            - mountPath: /etc/provisioner/config
              name: provisioner-config
              readOnly: true
            - mountPath:  /mnt/disks
              name: e2e-default
              mountPropagation: Bidirectional
            - mountPath: /bin/entrypoint.sh
              name: setup-disks
              subPath: entrypoint.sh
      volumes:
        - name: provisioner-config
          configMap:
            name: local-provisioner-config
        - name: e2e-default
          hostPath:
            path: /mnt/disks
        - name: setup-disks
          configMap:
            defaultMode: 0700
            name: setup-disks
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: local-storage-admin
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: local-storage-provisioner-pv-binding
  namespace: default
subjects:
- kind: ServiceAccount
  name: local-storage-admin
  namespace: default
roleRef:
  kind: ClusterRole
  name: system:persistent-volume-provisioner
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: local-storage-provisioner-node-clusterrole
  namespace: default
rules:
- apiGroups:
  - extensions
  resources:
  - podsecuritypolicies
  resourceNames:
  - gce.privileged
  verbs:
  - use
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: local-storage-provisioner-node-binding
  namespace: default
subjects:
- kind: ServiceAccount
  name: local-storage-admin
  namespace: default
roleRef:
  kind: ClusterRole
  name: local-storage-provisioner-node-clusterrole
  apiGroup: rbac.authorization.k8s.io