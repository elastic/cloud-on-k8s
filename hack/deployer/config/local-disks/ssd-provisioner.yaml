apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: e2e-default
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
reclaimPolicy: Delete
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: local-provisioner-config
  namespace: default
data:
  useNodeNameOnly: "true"
  storageClassMap: |
    e2e-default:
       hostDir: /mnt/disks/pvs
       mountDir:  /mnt/disks/pvs
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: setup-disks
data:
  format.sh: |-
    #!/bin/bash
    # Mounts Amazon EC2 NVMe instance storage as ext4 to /mnt/ssd[0-9]
    # Inspired by https://github.com/brunsgaard/eks-nvme-ssd-provisioner/blob/master/eks-nvme-ssd-provisioner.sh

    set -o errexit
    set -o nounset
    set -o pipefail

    # TODO custom docker image
    apt-get update && apt-get -y install nvme-cli e2fsprogs

    SSD_NVME_DEVICE_LIST=($(nvme list | grep "Amazon EC2 NVMe Instance Storage" | cut -d " " -f 1 || true))
    SSD_NVME_DEVICE_COUNT=${#SSD_NVME_DEVICE_LIST[@]}
    FILESYSTEM_BLOCK_SIZE=${FILESYSTEM_BLOCK_SIZE:-4096}  # Bytes

    # Checking if provisioning already happend
    if [[ "$(ls -A /mnt/disks)" ]]
    then
      echo 'Volumes already present in "/mnt/disks"'
      echo -e "\n$(ls -Al /mnt/disks | tail -n +2)\n"
      echo "I assume that provisioning already happend, doing nothing!"
      exit 0
    fi

    # Perform provisioning based on nvme device count
    case $SSD_NVME_DEVICE_COUNT in
    "0")
      echo 'No devices found of type "Amazon EC2 NVMe Instance Storage"'
      exit 1
      ;;
    *)
      for (( i=0; i<${SSD_NVME_DEVICE_COUNT}; i++ ));
      do
        DEVICE=${SSD_NVME_DEVICE_LIST[$i]}
        mkfs.ext4 -m 0 -b $FILESYSTEM_BLOCK_SIZE $DEVICE
        DIR=ssd$i
        MNTPOINT=/mnt/disks/$DIR
        UUID=$(blkid -s UUID -o value $DEVICE)
        mkdir -p $MNTPOINT
        mount -o defaults,noatime,discard,nobarrier --uuid $UUID $MNTPOINT
      done
      ;;
    esac

    echo "NVMe SSD provisioning is done"
  entrypoint.sh: |-
    #!/bin/sh
    # Each PV is a fixed-size (default 10Gi) loop-backed ext4 filesystem so that df (and thus
    # "remaining space" as seen by Pods) reflects the constraint, not the whole disk.
    # Requires the first NVMe instance store device to have at least (PV_COUNT * PV_SIZE_GB) GiB
    # free (e.g. 110Gi for default 10x11Gi). No runtime check is performed; use instance types
    # with sufficient NVMe capacity (e.g. c5d.2xlarge has 200GB).
    # PV_SIZE_GB is the backing file size; usable space is ~10Gi after ext4 metadata (default 11G).
    # IMPORTANT: truncate creates sparse files: they report size PV_SIZE_GB but do not allocate that much
    # on disk upfront. Actual disk usage grows only as data is written (via the loop-mounted
    # ext4). So e.g. 10Ã—11G can fit on a 64G volume (e.g. Azure temp disk) until usage grows.
    set -e
    PV_SIZE_GB="${PV_SIZE_GB:-11}"
    PV_COUNT="${PV_COUNT:-10}"
    # When format.sh is skipped (GKE/AKS kustomize overlay), ssd0 does not exist; pvs is created by the loop.
    mkdir -p /mnt/disks/ssd0
    for i in $(seq 1 $PV_COUNT); do
      img="/mnt/disks/ssd0/disk-$i.img"
      mount_point="/mnt/disks/pvs/pv-$i"
      echo "Setting up pv-$i (${PV_SIZE_GB}Gi per PV)"
      mkdir -p "${mount_point}"
      if mountpoint -q -- "${mount_point}"; then
        echo "${mount_point} already mounted"
        continue
      fi
      if [ ! -f "${img}" ]; then
        echo "Creating sparse file ${img} size ${PV_SIZE_GB}G"
        truncate -s "${PV_SIZE_GB}G" "${img}"
        loop=$(losetup -f --show "${img}")
        echo "Formatting ${loop} as ext4"
        mkfs.ext4 -m 0 -F "${loop}"
        losetup -d "${loop}"
      fi
      loop=$(losetup -f --show "${img}")
      mount -o defaults,noatime,nobarrier "${loop}" "${mount_point}"
      echo "Mounted ${loop} at ${mount_point}"
    done
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: local-volume-provisioner
  namespace: default
  labels:
    app: local-volume-provisioner
spec:
  selector:
    matchLabels:
      app: local-volume-provisioner
  template:
    metadata:
      labels:
        app: local-volume-provisioner
    spec:
      serviceAccountName: local-storage-admin
      initContainers:
      - name: disk-format
        image: debian:stable-slim
        securityContext:
          privileged: true
        command:
        - "/bin/format.sh"
        volumeMounts:
          - mountPath: /bin/format.sh
            name: setup-disks
            subPath: format.sh
          - mountPath:  /mnt/disks
            name: e2e-default
            mountPropagation: Bidirectional
      - name: disk-mounts
        image: alpine:3.22
        securityContext:
          privileged: true
        command:
        - sh
        - -c
        - "apk add --no-cache e2fsprogs util-linux && /bin/entrypoint.sh"
        env:
        - name: PV_SIZE_GB
          value: "11"
        - name: PV_COUNT
          value: "10"
        volumeMounts:
          - mountPath: /bin/entrypoint.sh
            name: setup-disks
            subPath: entrypoint.sh
          - mountPath: /mnt/disks
            name: e2e-default
            mountPropagation: Bidirectional
          - mountPath: /dev
            name: host-dev
      containers:
        - image: "k8s.gcr.io/sig-storage/local-volume-provisioner:v2.4.0"
          imagePullPolicy: "Always"
          name: provisioner
          securityContext:
            privileged: true
          env:
          - name: MY_NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          volumeMounts:
            - mountPath: /etc/provisioner/config
              name: provisioner-config
              readOnly: true
            - mountPath:  /mnt/disks
              name: e2e-default
              mountPropagation: Bidirectional
            - mountPath: /bin/entrypoint.sh
              name: setup-disks
              subPath: entrypoint.sh
      volumes:
        - name: provisioner-config
          configMap:
            name: local-provisioner-config
        - name: e2e-default
          hostPath:
            path: /mnt/disks
        - name: setup-disks
          configMap:
            defaultMode: 0700
            name: setup-disks
        - name: host-dev
          hostPath:
            path: /dev
            type: Directory
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: local-storage-admin
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: local-storage-provisioner-pv-binding
  namespace: default
subjects:
- kind: ServiceAccount
  name: local-storage-admin
  namespace: default
roleRef:
  kind: ClusterRole
  name: system:persistent-volume-provisioner
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: local-storage-provisioner-node-clusterrole
  namespace: default
rules:
- apiGroups:
  - extensions
  resources:
  - podsecuritypolicies
  resourceNames:
  - gce.privileged
  verbs:
  - use
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: local-storage-provisioner-node-binding
  namespace: default
subjects:
- kind: ServiceAccount
  name: local-storage-admin
  namespace: default
roleRef:
  kind: ClusterRole
  name: local-storage-provisioner-node-clusterrole
  apiGroup: rbac.authorization.k8s.io