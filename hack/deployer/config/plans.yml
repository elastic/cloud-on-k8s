plans:
- id: gke-ci
  operation: create
  clusterName: ci
  provider: gke
  kubernetesVersion: 1.20
  machineType: n1-standard-8
  serviceAccount: true
  psp: true
  # use kustomize in GKE to remove the NVMe provisioning already taken care of by the platform
  diskSetup: kubectl apply -k hack/deployer/config/local-disks
  gke:
    region: europe-west2
    localSsdCount: 1
    nodeCountPerZone: 1
    gcpScopes: https://www.googleapis.com/auth/devstorage.read_only,https://www.googleapis.com/auth/logging.write,https://www.googleapis.com/auth/monitoring,https://www.googleapis.com/auth/servicecontrol,https://www.googleapis.com/auth/service.management.readonly,https://www.googleapis.com/auth/trace.append
- id: gke-dev
  operation: create
  clusterName: dev
  provider: gke
  kubernetesVersion: 1.20
  machineType: n1-standard-8
  serviceAccount: false
  psp: false
  gke:
    region: europe-west1
    localSsdCount: 1
    nodeCountPerZone: 1
    # Uncomment option below to enable network policy enforcement in GKE.
    # networkPolicy: true
    # Uncomment option below to create a private GKE cluster.
    # Note that when a cluster is private you must:
    #   1. Create a firewall rule so that the webhook can be accessed from the API server (see https://github.com/elastic/cloud-on-k8s/issues/1673#issuecomment-528449682)
    #   2. Create a VM to access the subnet and authorize the VM to access the master, see https://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters#private_master
    # private: true
    # gke creates a secondary IP range for all Pods of a cluster
    # gke defaults to a /14 subnet, which allows 262k Pods per cluster, but only 62 subnets to be created
    # /20 allows 4094 subnets, with up to 4094 IPs (Pods) per subnet
    # more clusters can therefore be created in the same VPC network.
    # we set a default of /20 that can be overridden here
    # clusterIpv4Cidr: /20
    # servicesIpv4Cidr: /20
    gcpScopes: https://www.googleapis.com/auth/devstorage.read_only,https://www.googleapis.com/auth/logging.write,https://www.googleapis.com/auth/monitoring,https://www.googleapis.com/auth/servicecontrol,https://www.googleapis.com/auth/service.management.readonly,https://www.googleapis.com/auth/trace.append
- id: aks-ci
  operation: create
  clusterName: ci
  provider: aks
  kubernetesVersion: 1.22.6
  machineType: Standard_D8s_v3
  serviceAccount: true
  psp: false
  diskSetup: kubectl apply -k hack/deployer/config/local-disks
  aks:
    nodeCount: 3
    location: westeurope
    zones: "1 2 3"
- id: aks-dev
  operation: create
  clusterName: dev
  provider: aks
  kubernetesVersion: 1.22.6
  machineType: Standard_D8s_v3
  serviceAccount: false
  psp: false
  aks:
    nodeCount: 3
    location: northeurope
    zones: "1 2 3"
- id: ocp-ci
  operation: create
  clusterName: ci
  provider: ocp
  machineType: n1-standard-8
  serviceAccount: true
  ocp:
    region: europe-west2
    nodeCount: 3
- id: ocp-dev
  operation: create
  clusterName: dev
  clientVersion: 4.9.9
  provider: ocp
  machineType: n1-standard-8
  serviceAccount: true
  ocp:
    region: europe-west1
    nodeCount: 3
- id: ocp3-ci
  operation: create
  clusterName: ci
  provider: ocp3
  machineType: n1-standard-8
  serviceAccount: true
  ocp3:
    workerCount: 3
- id: ocp3-dev
  operation: create
  clusterName: dev
  provider: ocp3
  machineType: n1-standard-8
  serviceAccount: false
  ocp3:
    workerCount: 3
- id: eks-ci
  operation: create
  clusterName: ci
  provider: eks
  machineType: c5d.2xlarge
  serviceAccount: false
  kubernetesVersion: 1.20
  diskSetup: kubectl apply -f hack/deployer/config/local-disks/ssd-provisioner.yaml
  eks:
    region: ap-northeast-3
    nodeCount: 3
    nodeAMI: auto
- id: eks-arm-ci
  operation: create
  clusterName: arm-ci
  provider: eks
  machineType: m6gd.2xlarge
  serviceAccount: false
  kubernetesVersion: 1.20
  diskSetup: kubectl apply -f hack/deployer/config/local-disks/ssd-provisioner.yaml
  eks:
    region: eu-west-1
    nodeCount: 3
    nodeAMI: auto
- id: eks-dev
  operation: create
  clusterName: dev
  provider: eks
  machineType: c5d.2xlarge
  serviceAccount: false
  kubernetesVersion: 1.20
  eks:
    region: eu-west-2
    nodeCount: 3
    nodeAMI: auto
- id: e2e-monitor
  operation: create
  clusterName: e2e-monitor
  provider: gke
  kubernetesVersion: 1.15
  machineType: n1-standard-8
  serviceAccount: false
  psp: false
  gke:
    region: europe-west2
    localSsdCount: 1
    nodeCountPerZone: 1
    gcpScopes: https://www.googleapis.com/auth/devstorage.read_only,https://www.googleapis.com/auth/logging.write,https://www.googleapis.com/auth/monitoring,https://www.googleapis.com/auth/servicecontrol,https://www.googleapis.com/auth/service.management.readonly,https://www.googleapis.com/auth/trace.append
- id: kind-dev
  operation: create
  clusterName: eck
  clientVersion: 0.11.1
  provider: kind
  kubernetesVersion: 1.21.1
  kind:
    nodeCount: 3
    nodeImage: kindest/node:v1.21.1@sha256:69860bda5563ac81e3c0057d654b5253219618a22ec3a346306239bba8cfa1a6
    ipFamily: ipv4
- id: kind-ci
  operation: create
  clusterName: kind-ci
  clientVersion: 0.11.1
  provider: kind
  kubernetesVersion: 1.21.1
  kind:
    nodeCount: 3
    nodeImage: kindest/node:v1.22.0@sha256:b8bda84bb3a190e6e028b1760d277454a72267a5454b57db34437c34a588d047
    ipFamily: ipv4
- id: tanzu-ci
  operation: create
  clusterName: tkg-ci
  provider: tanzu
  machineType: Standard_D8s_v3
  diskSetup: kubectl apply -k hack/deployer/config/local-disks
  tanzu:
    location: eastus
    nodeCount: 3
    installerImage: cloudonk8s.azurecr.io/tanzu-ci:47a15de6827f
- id: tanzu-dev
  operation: create
  clusterName: tkg-dev
  provider: tanzu
  machineType: Standard_D8s_v3
  tanzu:
    location: westeurope
    nodeCount: 3
    installerImage: cloudonk8s.azurecr.io/tanzu-ci:47a15de6827f
